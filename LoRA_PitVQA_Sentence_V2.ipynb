{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/LoRA_PitVQA_Sentence_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e3HXmQrS6QV",
        "outputId": "84b60982-8f00-4528-92f6-e1827c348ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PitVQA'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 401 (delta 74), reused 0 (delta 0), pack-reused 262 (from 1)\u001b[K\n",
            "Receiving objects: 100% (401/401), 14.44 MiB | 12.38 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n",
            "/content/PitVQA/datasets\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
            "From (redirected): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78&confirm=t&uuid=65eea26c-adf6-4f92-b683-42ecce0695d1\n",
            "To: /content/PitVQA/datasets/EndoVis-18-VQA.zip\n",
            "100% 2.71G/2.71G [00:49<00:00, 54.1MB/s]\n",
            "/content/PitVQA\n"
          ]
        }
      ],
      "source": [
        "#Download code\n",
        "!git clone https://github.com/HRL-Mike/PitVQA.git\n",
        "\n",
        "#Download Dataset\n",
        "!mkdir /content/PitVQA/datasets\n",
        "%cd /content/PitVQA/datasets\n",
        "!gdown --id 1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\n",
        "!unzip -q EndoVis-18-VQA.zip\n",
        "%cd /content/PitVQA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm==0.9.12 fairscale==0.4.13 scikit-learn==1.3.2 -U evaluate bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOkwHQeZTOZi",
        "outputId": "ea9fdee6-33ca-4057-f5eb-904f7fdd061a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "zn1LPGY3UPnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "class EndoVis18VQAGPTGen(Dataset):\n",
        "    def __init__(self, seq, folder_head, folder_tail):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),  # input image size\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq:\n",
        "            filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' % (len(filenames), len(self.vqas)))\n",
        "\n",
        "        # Labels\n",
        "        self.labels = ['kidney',\n",
        "                'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa_full_path = Path(self.vqas[idx][0])\n",
        "        seq_path = qa_full_path.parents[2]\n",
        "        file_name = self.vqas[idx][0].split('/')[-1]  # / in linux and \\\\ in windows\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(seq_path, 'left_fr', file_name.split('_')[0] + '.png')\n",
        "        raw_image = Image.open(img_loc).convert('RGB')\n",
        "        img = self.transform(raw_image)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "\n",
        "        return img_loc, img, question, answer"
      ],
      "metadata": {
        "id": "-6kiE739TSFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "j9ySe-NcULLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import ViTModel, BlipConfig, BlipTextModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class _LoRA_qkv(nn.Module):\n",
        "    def __init__(self, w_qkv, w_a, w_b, lora_alpha, lora_dropout):\n",
        "        super().__init__()\n",
        "        self.w_qkv = w_qkv\n",
        "        self.w_a = w_a\n",
        "        self.w_b = w_b\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.dropout = nn.Dropout(lora_dropout)\n",
        "        self.scaling = self.lora_alpha / self.w_a.weight.shape[0]  # alpha / r\n",
        "\n",
        "        self.weight = self.w_qkv.weight  # load original weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_qkv(x) + self.scaling * self.dropout(self.w_b(self.w_a(x)))\n",
        "\n",
        "class LoRAInitializer:\n",
        "    def __init__(self, model, r=None, lora=None, lora_alpha=32, lora_dropout=0.1):\n",
        "        if r is None:\n",
        "            r = [14, 14, 12, 12, 10, 10, 8, 8, 8, 8, 8, 8]\n",
        "        if lora is None:\n",
        "            lora = ['q', 'v']\n",
        "\n",
        "        self.model = model\n",
        "        self.r = r\n",
        "        self.lora = lora\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.lora_dropout = lora_dropout\n",
        "        self.w_As = []\n",
        "        self.w_Bs = []\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for w_A, w_B in zip(self.w_As, self.w_Bs):\n",
        "            # normal distribution init for w_A\n",
        "            nn.init.normal_(w_A.weight, mean=0.0, std=0.02)\n",
        "            nn.init.zeros_(w_B.weight)  # zero init for w_B\n",
        "\n",
        "    def initialize_lora(self):\n",
        "        for param in self.model.transformer.parameters():\n",
        "            param.requires_grad = False  # freeze transformer parameters\n",
        "            # param.requires_grad = True\n",
        "\n",
        "        for t_layer_i, blk in enumerate(self.model.transformer.h):  # t_layer_i = [0, 11], blk = transformer block\n",
        "            # GPT2 uses a single c_attn for q, k, v\n",
        "            w_qkv = blk.attn.c_attn\n",
        "            in_features = w_qkv.weight.shape[0]  # 768\n",
        "            out_features = w_qkv.weight.shape[1]  # 2304\n",
        "\n",
        "            w_a_linear = nn.Linear(in_features, self.r[t_layer_i], bias=False)\n",
        "            w_b_linear = nn.Linear(self.r[t_layer_i], out_features, bias=False)\n",
        "            self.w_As.append(w_a_linear)\n",
        "            self.w_Bs.append(w_b_linear)\n",
        "            blk.attn.c_attn = _LoRA_qkv(w_qkv, w_a_linear, w_b_linear, self.lora_alpha, self.lora_dropout)\n",
        "\n",
        "        self.reset_parameters()\n",
        "        print(\"LoRA params initialized!\")\n",
        "        return self.model\n",
        "\n",
        "\n",
        "class BLIPGPTVQAGen(nn.Module):\n",
        "    def __init__(self, r=None, lora=None, lora_alpha=32, lora_dropout=0.1):\n",
        "        super(BLIPGPTVQAGen, self).__init__()\n",
        "\n",
        "        # gpt2 decoder\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt = LoRAInitializer(self.gpt, r=r, lora=lora, lora_alpha=lora_alpha,\n",
        "                       lora_dropout=lora_dropout).initialize_lora()  # add lora\n",
        "\n",
        "        # visual encoder\n",
        "        model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "        self.visual_encoder = ViTModel.from_pretrained(model_name)\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # end of string\n",
        "\n",
        "        # text encoder\n",
        "        config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "        self.text_encoder = BlipTextModel(config.text_config, add_pooling_layer=False)\n",
        "\n",
        "        # modify embedding layer\n",
        "        new_vocab_size = len(self.tokenizer)\n",
        "        embedding_dim = self.text_encoder.embeddings.word_embeddings.embedding_dim\n",
        "        self.text_encoder.embeddings.word_embeddings = nn.Embedding(new_vocab_size, embedding_dim)  # He init\n",
        "\n",
        "    def forward(self, image, question_inputs, answer_inputs=None):\n",
        "        # visual encoder\n",
        "        image = image.to(device)\n",
        "        image_embeds = self.visual_encoder(image).last_hidden_state  # torch.Size([bs, 197, 768])\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)  # torch.Size([bs, 197])\n",
        "\n",
        "        question_input_ids = question_inputs['input_ids']  # torch.Size([bs, 25])\n",
        "        question_att_mask = question_inputs['attention_mask']\n",
        "\n",
        "        answer_input_ids = answer_inputs['input_ids']  # torch.Size([bs, 25])\n",
        "        answer_att_mask = answer_inputs['attention_mask']\n",
        "\n",
        "        # multimodal encoder\n",
        "        img_question_output = self.text_encoder(input_ids=question_input_ids,\n",
        "                         attention_mask=question_att_mask,\n",
        "                         encoder_hidden_states=image_embeds,\n",
        "                         encoder_attention_mask=image_atts,\n",
        "                         return_dict=True)\n",
        "\n",
        "        img_question_embeds = img_question_output.last_hidden_state  # torch.Size([bs, 25, 768]), args.question_len=25\n",
        "\n",
        "        # multimodal encoder\n",
        "        img_answer_output = self.text_encoder(input_ids=answer_input_ids,\n",
        "                         attention_mask=answer_att_mask,\n",
        "                         encoder_hidden_states=image_embeds,\n",
        "                         encoder_attention_mask=image_atts,\n",
        "                         return_dict=True)\n",
        "\n",
        "        img_answer_embeds = img_answer_output.last_hidden_state  # torch.Size([bs, 25, 768]), args.question_len=25\n",
        "        # print('img_answer_embeds:', img_answer_embeds.shape)\n",
        "\n",
        "        inputs_embeds_qa = torch.cat((img_question_embeds, img_answer_embeds), dim=1)\n",
        "        # print('inputs_embeds_qa:', inputs_embeds_qa.shape)\n",
        "\n",
        "        # text decoder\n",
        "        gpt_output = self.gpt(inputs_embeds=inputs_embeds_qa,\n",
        "                              encoder_attention_mask=question_att_mask)  # torch.Size([bs, 25, 50257])\n",
        "        return gpt_output.logits"
      ],
      "metadata": {
        "id": "MmRX4VGiTvU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "yF8S896pVrVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, (_, images, questions, answers) in enumerate(tqdm(train_dataloader), 0):\n",
        "        question_inputs = tokenizer(questions, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                    return_tensors=\"pt\", truncation=True)\n",
        "        answer_inputs = tokenizer(answers, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                  return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "        # get logits and labels\n",
        "        logits = model(image=images.to(device), question_inputs=question_inputs.to(device), answer_inputs=answer_inputs.to(device))\n",
        "        labels = answer_inputs['input_ids'].to(device)\n",
        "\n",
        "        # print('logit:', logits.shape)\n",
        "        # get shifted logits and labels\n",
        "        shift_logits = logits[:, args.seq_length:, :].contiguous()\n",
        "        shift_labels = labels[:, :].contiguous()\n",
        "        # print('shift_logits:', shift_logits.shape)\n",
        "        # print('shift_labels:', shift_labels.shape)\n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        for i, (_, images, questions, answers) in enumerate(tqdm(val_loader), 0):\n",
        "            question_inputs = tokenizer(questions, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                        return_tensors=\"pt\", truncation=True)\n",
        "            answer_inputs = tokenizer(answers, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                      return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "            # get logits and labels\n",
        "            logits = model(image=images.to(device), question_inputs=question_inputs.to(device), answer_inputs=answer_inputs.to(device))\n",
        "            labels = answer_inputs['input_ids'].to(device)\n",
        "\n",
        "            # get shifted logits and labels\n",
        "            shift_logits = logits[:, args.seq_length:, :].contiguous()\n",
        "            shift_labels = labels[:, :].contiguous()\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # generate predicted answer\n",
        "            _, predicted = torch.max(shift_logits, dim=-1)\n",
        "\n",
        "            # decode references and predictions\n",
        "            reference_answers = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predicted_answers = tokenizer.batch_decode(predicted, skip_special_tokens=True)\n",
        "            # print('reference_answers:', reference_answers)\n",
        "            # add references and hypotheses to lists\n",
        "            for ref, hyp in zip(reference_answers, predicted_answers):\n",
        "                references.append([ref.split()])\n",
        "                hypotheses.append(hyp.split())\n",
        "\n",
        "\n",
        "        # Calculate BLEU_1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        print(f\"Epoch: {epoch}/{args.epochs} EVA LOSS: {total_loss.avg:.6f} \"\n",
        "              f\"BLEU-1: {metrics['Bleu_1']:.6f} BLEU-2: {metrics['Bleu_2']:.6f} \"\n",
        "              f\"BLEU-3: {metrics['Bleu_3']:.6f} BLEU-4: {metrics['Bleu_4']:.6f}\")\n",
        "    return metrics\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.epochs = 20\n",
        "        self.batch_size = 8\n",
        "        self.workers = 8\n",
        "        self.random_seed = 42\n",
        "        self.seq_length = 32\n",
        "        self.lr = 0.00002\n",
        "\n",
        "        self.vector_rank = [14, 14, 12, 12, 10, 10, 8, 8, 8, 8, 8, 8]\n",
        "        self.lora_alpha = 32\n",
        "        self.lora_dropout = 0.1\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     args = Args()\n",
        "#     os.makedirs('./checkpoints/', exist_ok=True)\n",
        "\n",
        "#     seed_everything(args.random_seed)\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     start_epoch = 1\n",
        "#     best_epoch = [0]\n",
        "#     best_results = [0.0]\n",
        "#     epochs_since_improvement = 0\n",
        "\n",
        "#     # data location\n",
        "#     train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "#     val_seq = [1, 5, 16]\n",
        "\n",
        "#     folder_head = '/content/PitVQA/datasets/EndoVis-18-VQA/seq_'\n",
        "#     folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "#     # dataloader\n",
        "#     train_dataset = EndoVis18VQAGPTGen(train_seq, folder_head, folder_tail)\n",
        "#     train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "#     val_dataset = EndoVis18VQAGPTGen(val_seq, folder_head, folder_tail)\n",
        "#     val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "#     print(f'num of elements: {len(args.vector_rank)}')\n",
        "#     model = BLIPGPTVQAGen(r=args.vector_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  # same learning rate for LoRA weights and other weights\n",
        "\n",
        "#     model = model.to(device)\n",
        "#     pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "#     print('model params: ', pytorch_total_params)\n",
        "#     criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#     print('Start training.')\n",
        "#     for epoch in range(start_epoch, args.epochs+1):\n",
        "\n",
        "#         if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "#             adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "#         # train\n",
        "#         train(args, train_dataloader=train_dataloader, model=model, criterion=criterion, optimizer=optimizer,\n",
        "#               epoch=epoch, tokenizer=tokenizer, device=device)\n",
        "\n",
        "#         # validation\n",
        "#         metrics = validate(args, val_loader=val_dataloader, model=model, criterion=criterion, epoch=epoch,\n",
        "#                            tokenizer=tokenizer, device=device)\n",
        "\n",
        "#         if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "#             epochs_since_improvement = 0\n",
        "#             best_results[0] = metrics[\"Bleu_4\"]\n",
        "#             best_epoch[0] = epoch\n",
        "#             print(f'Best epoch: {epoch}, Best Bleu_4: {metrics[\"Bleu_4\"]}')\n",
        "#             torch.save(model.state_dict(), 'checkpoints/model_best.pth')\n",
        "#         else:\n",
        "#             epochs_since_improvement += 1\n",
        "#             print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "#     print('End training.')"
      ],
      "metadata": {
        "id": "MQBpj1WhUqgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference:"
      ],
      "metadata": {
        "id": "uNiUNJvzFRCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading trained weights\n",
        "!gdown 1G9V2G-ZkfyswsKAdYCqUoNjiYIKNqOcg"
      ],
      "metadata": {
        "id": "S1lSFNhlFTgG",
        "outputId": "eaf01517-407a-46b2-b246-c2358af1d681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1G9V2G-ZkfyswsKAdYCqUoNjiYIKNqOcg\n",
            "From (redirected): https://drive.google.com/uc?id=1G9V2G-ZkfyswsKAdYCqUoNjiYIKNqOcg&confirm=t&uuid=08f230ed-8fb1-44ce-8d76-32f0928de9b6\n",
            "To: /content/PitVQA/model_best.pth\n",
            "100% 1.45G/1.45G [00:28<00:00, 51.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gdo-D621MkMX",
        "outputId": "eaa9d7d7-7c52-4738-d203-60ba4d149831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'collections.OrderedDict' object has no attribute 'to'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-98b09fb686a0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/PitVQA/model_best.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import ViTModel, BlipConfig, BlipTextModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class _LoRA_qkv(nn.Module):\n",
        "    def __init__(self, w_qkv, w_a, w_b, lora_alpha, lora_dropout):\n",
        "        super().__init__()\n",
        "        self.w_qkv = w_qkv\n",
        "        self.w_a = w_a\n",
        "        self.w_b = w_b\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.dropout = nn.Dropout(lora_dropout)\n",
        "        self.scaling = self.lora_alpha / self.w_a.weight.shape[0]  # alpha / r\n",
        "\n",
        "        self.weight = self.w_qkv.weight  # load original weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_qkv(x) + self.scaling * self.dropout(self.w_b(self.w_a(x)))\n",
        "\n",
        "class LoRAInitializer:\n",
        "    def __init__(self, model, r=None, lora=None, lora_alpha=32, lora_dropout=0.1):\n",
        "        if r is None:\n",
        "            r = [14, 14, 12, 12, 10, 10, 8, 8, 8, 8, 8, 8]\n",
        "        if lora is None:\n",
        "            lora = ['q', 'v']\n",
        "\n",
        "        self.model = model\n",
        "        self.r = r\n",
        "        self.lora = lora\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.lora_dropout = lora_dropout\n",
        "        self.w_As = []\n",
        "        self.w_Bs = []\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for w_A, w_B in zip(self.w_As, self.w_Bs):\n",
        "            # normal distribution init for w_A\n",
        "            nn.init.normal_(w_A.weight, mean=0.0, std=0.02)\n",
        "            nn.init.zeros_(w_B.weight)  # zero init for w_B\n",
        "\n",
        "    def initialize_lora(self):\n",
        "        for param in self.model.transformer.parameters():\n",
        "            param.requires_grad = False  # freeze transformer parameters\n",
        "            # param.requires_grad = True\n",
        "\n",
        "        for t_layer_i, blk in enumerate(self.model.transformer.h):  # t_layer_i = [0, 11], blk = transformer block\n",
        "            # GPT2 uses a single c_attn for q, k, v\n",
        "            w_qkv = blk.attn.c_attn\n",
        "            in_features = w_qkv.weight.shape[0]  # 768\n",
        "            out_features = w_qkv.weight.shape[1]  # 2304\n",
        "\n",
        "            w_a_linear = nn.Linear(in_features, self.r[t_layer_i], bias=False)\n",
        "            w_b_linear = nn.Linear(self.r[t_layer_i], out_features, bias=False)\n",
        "            self.w_As.append(w_a_linear)\n",
        "            self.w_Bs.append(w_b_linear)\n",
        "            blk.attn.c_attn = _LoRA_qkv(w_qkv, w_a_linear, w_b_linear, self.lora_alpha, self.lora_dropout)\n",
        "\n",
        "        self.reset_parameters()\n",
        "        print(\"LoRA params initialized!\")\n",
        "        return self.model\n",
        "\n",
        "\n",
        "class BLIPGPTVQAGen(nn.Module):\n",
        "    def __init__(self, r=None, lora=None, lora_alpha=32, lora_dropout=0.1):\n",
        "        super(BLIPGPTVQAGen, self).__init__()\n",
        "\n",
        "        # gpt2 decoder\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt = LoRAInitializer(self.gpt, r=r, lora=lora, lora_alpha=lora_alpha,\n",
        "                       lora_dropout=lora_dropout).initialize_lora()  # add lora\n",
        "\n",
        "        # visual encoder\n",
        "        model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "        self.visual_encoder = ViTModel.from_pretrained(model_name)\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # end of string\n",
        "\n",
        "        # text encoder\n",
        "        config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "        self.text_encoder = BlipTextModel(config.text_config, add_pooling_layer=False)\n",
        "\n",
        "        # modify embedding layer\n",
        "        new_vocab_size = len(self.tokenizer)\n",
        "        embedding_dim = self.text_encoder.embeddings.word_embeddings.embedding_dim\n",
        "        self.text_encoder.embeddings.word_embeddings = nn.Embedding(new_vocab_size, embedding_dim)  # He init\n",
        "\n",
        "    def forward(self, image, question_inputs, answer_inputs=None):\n",
        "        # visual encoder\n",
        "        image = image.to(device)\n",
        "        image_embeds = self.visual_encoder(image).last_hidden_state  # torch.Size([bs, 197, 768])\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)  # torch.Size([bs, 197])\n",
        "\n",
        "        question_input_ids = question_inputs['input_ids']  # torch.Size([bs, 25])\n",
        "        question_att_mask = question_inputs['attention_mask']\n",
        "\n",
        "\n",
        "        # multimodal encoder\n",
        "        img_question_output = self.text_encoder(input_ids=question_input_ids,\n",
        "                         attention_mask=question_att_mask,\n",
        "                         encoder_hidden_states=image_embeds,\n",
        "                         encoder_attention_mask=image_atts,\n",
        "                         return_dict=True)\n",
        "\n",
        "        inputs_embeds_qa = img_question_output.last_hidden_state  # torch.Size([bs, 25, 768]), args.question_len=25\n",
        "\n",
        "        # multimodal encoder\n",
        "        if answer_inputs is not None:\n",
        "            answer_input_ids = answer_inputs['input_ids']  # torch.Size([bs, 25])\n",
        "            answer_att_mask = answer_inputs['attention_mask']\n",
        "\n",
        "            img_answer_output = self.text_encoder(input_ids=answer_input_ids,\n",
        "                            attention_mask=answer_att_mask,\n",
        "                            encoder_hidden_states=image_embeds,\n",
        "                            encoder_attention_mask=image_atts,\n",
        "                            return_dict=True)\n",
        "\n",
        "            img_answer_embeds = img_answer_output.last_hidden_state  # torch.Size([bs, 25, 768]), args.question_len=25\n",
        "            # print('img_answer_embeds:', img_answer_embeds.shape)\n",
        "\n",
        "            inputs_embeds_qa = torch.cat((inputs_embeds_qa, img_answer_embeds), dim=1)\n",
        "            # print('inputs_embeds_qa:', inputs_embeds_qa.shape)\n",
        "\n",
        "        print('inputs_embeds_qa:',inputs_embeds_qa.shape)\n",
        "        # text decoder\n",
        "        gpt_output = self.gpt(inputs_embeds=inputs_embeds_qa,\n",
        "                              encoder_attention_mask=question_att_mask)  # torch.Size([bs, 25, 50257])\n",
        "        return gpt_output.logits\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = Args()\n",
        "    os.makedirs('./checkpoints/', exist_ok=True)\n",
        "\n",
        "    seed_everything(args.random_seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = '/content/PitVQA/datasets/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    # dataloader\n",
        "    train_dataset = EndoVis18VQAGPTGen(train_seq, folder_head, folder_tail)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    val_dataset = EndoVis18VQAGPTGen(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f'num of elements: {len(args.vector_rank)}')\n",
        "    model = BLIPGPTVQAGen(r=args.vector_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  # same learning rate for LoRA weights and other weights\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    model.load_state_dict(torch.load('/content/PitVQA/model_best.pth'))\n",
        "\n",
        "\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n"
      ],
      "metadata": {
        "id": "KHBu3JlM5yt3",
        "outputId": "c7a5845f-afd3-494e-c37f-9264b05859ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "num of elements: 12\n",
            "LoRA params initialized!\n",
            "model params:  363611136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (_, images, questions, answers) in enumerate(tqdm(val_dataloader), 0):\n",
        "    question_inputs = tokenizer(questions, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                return_tensors=\"pt\", truncation=True)\n",
        "    answer_inputs = tokenizer(answers, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                return_tensors=\"pt\", truncation=True)\n",
        "    print('questions:', questions)\n",
        "    print('answers:', answers)\n",
        "    print('question_inputs:', question_inputs['input_ids'].shape)\n",
        "    logits = model(image=images.to(device), question_inputs=question_inputs.to(device))\n",
        "    print('logits:', logits.shape)\n",
        "    _, predicted = torch.max(logits, dim=-1)\n",
        "    predicted_answers = tokenizer.batch_decode(predicted, skip_special_tokens=True)\n",
        "    print('predicted_answers:', predicted_answers)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "RCaf84Z37W9C",
        "outputId": "1895b6e3-604d-4802-b8a5-8bd8d69e91dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3216 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "questions: ('what organ is being operated?',)\n",
            "answers: ('organ being operated is kidney',)\n",
            "question_inputs: torch.Size([1, 32])\n",
            "inputs_embeds_qa: torch.Size([1, 32, 768])\n",
            "logits: torch.Size([1, 32, 50257])\n",
            "predicted_answers: ['-ates is being operated to']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3216 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tIbEquGk7XJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "def treebank_tokenize(s):\n",
        "    return TreebankWordTokenizer().tokenize(s)\n",
        "def generate_beam(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    beam_size: int = 5,\n",
        "    generated=None,\n",
        "    entry_length=65,\n",
        "    temperature=1.0,\n",
        "    stop_token: str = \"<|endoftext|>\",\n",
        "):\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "            logits = logits.softmax(-1).log()\n",
        "            # final_logit\n",
        "\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(\n",
        "                    beam_size, -1\n",
        "                )\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            if model.model_type == \"biogpt\":\n",
        "                next_token_embed = model.gpt.biogpt.embed_tokens(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            elif model.model_type == \"gpt2\":\n",
        "                next_token_embed = model.gpt.transformer.wte(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            else:\n",
        "                next_token_embed = model.gpt.get_input_embeddings()(tokens[:,-1])\n",
        "                next_token_embed=next_token_embed.squeeze().view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [\n",
        "        tokenizer.decode(output[: int(length)])\n",
        "        for output, length in zip(output_list, seq_lengths)\n",
        "    ]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "# from utils import generate_beam\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import GPT2Tokenizer\n",
        "import pdb\n",
        "from evaluate import load\n",
        "import collections\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "\n",
        "def print_nearest_text_token(vis_token, model):\n",
        "    \"\"\"print the nearest token in the vocabulary to the given token through model.gpt.embeddings.weight\"\"\"\n",
        "    embeddings = model.gpt.transformer.wte.weight\n",
        "    distances = torch.norm(embeddings - vis_token, dim=1)\n",
        "    nearest_token_idx = torch.argmin(distances)\n",
        "    print(model.tokenizer.decode([nearest_token_idx.item()]))\n",
        "\n",
        "def compute_f1(gold_toks, pred_toks):\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def eval_gpt_open_ended(model, dataset, args, print_vis_token_meaning=False):\n",
        "    model.eval()\n",
        "    model=model.cuda()\n",
        "    bert_score = load(\"bertscore\")\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model.model_type)\n",
        "    bleu_avg1=0.\n",
        "    bert_avg1 = 0.\n",
        "    bert_avg2 = 0.\n",
        "    bert_avg3 = 0.\n",
        "    f1_avg = 0.\n",
        "    acc = 0.\n",
        "    acc_oe = 0.\n",
        "    acc_yn = 0.\n",
        "    c_oe =1e-9\n",
        "    c_yn =1e-9\n",
        "    with tqdm(total=len(dataset)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(\"Testing\")\n",
        "        for item in range(len(dataset)):\n",
        "            prefix,  labels, tokens, mask, q_len = dataset[item]\n",
        "            prefix = prefix.type(torch.float32).cuda()\n",
        "            tokens = tokens.type(torch.long).cuda()\n",
        "            mask = mask.cuda()\n",
        "            with autocast(dtype=torch.float16):\n",
        "              with torch.no_grad():\n",
        "                  embed = model.generate(prefix,labels,tokens,mask,q_len).view(1,tokens.size(0),-1)\n",
        "                  if print_vis_token_meaning:\n",
        "                    prefix_projections = embed[:,q_len:q_len+model.prefix_length,:]\n",
        "                    for i in range(prefix_projections.size(1)):\n",
        "                      print_nearest_text_token(prefix_projections[0,i], model)\n",
        "                  out_text = generate_beam(model, model.tokenizer,generated=embed,entry_length=dataset.max_seqs_len[1], temperature=1)[0]\n",
        "\n",
        "            if out_text.lower()==dataset.answers[item].lower():\n",
        "              acc+=1\n",
        "            if dataset.answers[item].lower()=='yes' or dataset.answers[item].lower()=='no':\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_yn+=1\n",
        "              c_yn+=1\n",
        "            else:\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_oe+=1\n",
        "              c_oe+=1\n",
        "\n",
        "            reference = [str(dataset.answers[item])]\n",
        "            candidate = [out_text]\n",
        "\n",
        "            bleu_1 = sentence_bleu(reference[0], candidate[0], weights=(1, 0, 0, 0))\n",
        "\n",
        "            a = bert_score.compute(references = reference,predictions = candidate,model_type = 'bert-base-uncased')\n",
        "            bert_avg1+= a['precision'][0]\n",
        "            bert_avg2+= a['recall'][0]\n",
        "            bert_avg3+= a['f1'][0]\n",
        "\n",
        "\n",
        "            f1_avg += compute_f1(tokenizer.encode(reference[0]),tokenizer.encode(candidate[0]))\n",
        "            bleu_avg1+=bleu_1\n",
        "\n",
        "\n",
        "    print('------------')\n",
        "    print(\"BLEU {}\".format(round(bleu_avg1/len(dataset),3)))\n",
        "    print(\"BERTScore {}\".format(round(bert_avg3/len(dataset),3)))\n",
        "    print(\"F1 {}\".format(round(f1_avg/len(dataset),3)))\n",
        "    print(\"Accuracy {}\".format(round(acc/len(dataset),3)))\n",
        "    print(\"Accuracy YN{}\".format(round(acc_yn/c_yn,3)))\n",
        "    print(\"Accuracy OE{}\".format(round(acc_oe/c_oe,3)))"
      ],
      "metadata": {
        "id": "5keX5EG4tgRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}