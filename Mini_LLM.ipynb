{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOogwctIHLTIUl6M6Or50bE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Mini_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://medium.com/@sntaus/building-a-mini-gpt-like-language-model-from-scratch-27257bf5c145 <br>"
      ],
      "metadata": {
        "id": "9Mk9t9_JuGVC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFia6vNqrU0h",
        "outputId": "7212a7ad-8e5a-457e-9460-d2b9260f30a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 12.3741\n",
            "Epoch 2, Loss: 12.2886\n",
            "Epoch 3, Loss: 12.1684\n",
            "Epoch 4, Loss: 12.0353\n",
            "Epoch 5, Loss: 11.8768\n",
            "Epoch 6, Loss: 11.7221\n",
            "Epoch 7, Loss: 11.5899\n",
            "Epoch 8, Loss: 11.4825\n",
            "Epoch 9, Loss: 11.4000\n",
            "Epoch 10, Loss: 11.3337\n",
            "Epoch 11, Loss: 11.2777\n",
            "Epoch 12, Loss: 11.2294\n",
            "Epoch 13, Loss: 11.1722\n",
            "Epoch 14, Loss: 11.1134\n",
            "Epoch 15, Loss: 11.0484\n",
            "Epoch 16, Loss: 10.9822\n",
            "Epoch 17, Loss: 10.9275\n",
            "Epoch 18, Loss: 10.8850\n",
            "Epoch 19, Loss: 10.8428\n",
            "Epoch 20, Loss: 10.7921\n",
            "Epoch 21, Loss: 10.7320\n",
            "Epoch 22, Loss: 10.6792\n",
            "Epoch 23, Loss: 10.6164\n",
            "Epoch 24, Loss: 10.5643\n",
            "Epoch 25, Loss: 10.5109\n",
            "Epoch 26, Loss: 10.4588\n",
            "Epoch 27, Loss: 10.4062\n",
            "Epoch 28, Loss: 10.3603\n",
            "Epoch 29, Loss: 10.3115\n",
            "Epoch 30, Loss: 10.2674\n",
            "Epoch 31, Loss: 10.2282\n",
            "Epoch 32, Loss: 10.1890\n",
            "Epoch 33, Loss: 10.1432\n",
            "Epoch 34, Loss: 10.0931\n",
            "Epoch 35, Loss: 10.0399\n",
            "Epoch 36, Loss: 9.9636\n",
            "Epoch 37, Loss: 9.9099\n",
            "Epoch 38, Loss: 9.8656\n",
            "Epoch 39, Loss: 9.8226\n",
            "Epoch 40, Loss: 9.7912\n",
            "Epoch 41, Loss: 9.7659\n",
            "Epoch 42, Loss: 9.7429\n",
            "Epoch 43, Loss: 9.7195\n",
            "Epoch 44, Loss: 9.6937\n",
            "Epoch 45, Loss: 9.6664\n",
            "Epoch 46, Loss: 9.6368\n",
            "Epoch 47, Loss: 9.6069\n",
            "Epoch 48, Loss: 9.5845\n",
            "Epoch 49, Loss: 9.5643\n",
            "Epoch 50, Loss: 9.5484\n",
            "Epoch 51, Loss: 9.5337\n",
            "Epoch 52, Loss: 9.5142\n",
            "Epoch 53, Loss: 9.4958\n",
            "Epoch 54, Loss: 9.4775\n",
            "Epoch 55, Loss: 9.4598\n",
            "Infering sequence 0\n",
            "Infering sequence 1\n",
            "Infering sequence 2\n",
            "Infering sequence 3\n",
            "Infering sequence 4\n",
            "Infering sequence 5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Data:\n",
            "{'how are you': 'i am fine <end>',\n",
            " 'how is john': 'i dont know <end>',\n",
            " 'where is john': 'at home <end>',\n",
            " 'who are you': 'mini gpt model <end>',\n",
            " 'who is john': 'a nice person <end>',\n",
            " 'who is nice': 'john <end>'}\n",
            "\n",
            "\n",
            "\n",
            "Model Inference:\n",
            "{'how are you': 'i am <end>',\n",
            " 'how is john': 'i <end>',\n",
            " 'where is john': 'at home <end>',\n",
            " 'who are you': 'mini gpt model <end>',\n",
            " 'who is john': 'a nice person <end>',\n",
            " 'who is nice': '<end>'}\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pprint\n",
        "\n",
        "# Function to obtain training data, vocab and mapping from word to index and vice versa\n",
        "def get_data_and_vocab():\n",
        "    # Define training data\n",
        "    training_data = {\n",
        "        \"how are you\": \"i am fine <end>\",\n",
        "        \"who is john\": \"a nice person <end>\",\n",
        "        \"who is nice\": \"john <end>\",\n",
        "        \"where is john\": \"at home <end>\",\n",
        "        \"how is john\": \"i dont know <end>\",\n",
        "        \"who are you\": \"mini gpt model <end>\"\n",
        "    }\n",
        "\n",
        "    # Extract input and target phrases\n",
        "    data_words = [k for k, _ in training_data.items()]\n",
        "    target_words = [v for _, v in training_data.items()]\n",
        "\n",
        "    # Build vocabulary from training data\n",
        "    vocabulary_words = list(set([element.lower() for nestedlist in [x.split(\" \") for x in data_words] for element in nestedlist] + [element.lower() for nestedlist in [x.split(\" \") for x in target_words] for element in nestedlist]))\n",
        "\n",
        "    # Ensure <end> token is at the end of vocabulary list, and there's a blank at the beginning\n",
        "    vocabulary_words.remove(\"<end>\")\n",
        "    vocabulary_words.append(\"<end>\")\n",
        "    vocabulary_words.insert(0, \"\")\n",
        "\n",
        "    # Create mappings from word to index and index to word\n",
        "    word_to_ix = {vocabulary_words[k].lower(): k for k in range(len(vocabulary_words))}\n",
        "    ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
        "\n",
        "    # Return all the necessary data and mappings\n",
        "    return training_data, data_words, target_words, vocabulary_words, word_to_ix, ix_to_word\n",
        "\n",
        "# Function to convert a batch of sequences of words to a tensor of indices\n",
        "def words_to_tensor(seq_batch, device=None):\n",
        "    index_batch = []\n",
        "\n",
        "    # Loop over sequences in the batch\n",
        "    for seq in seq_batch:\n",
        "        word_list = seq.lower().split(\" \")\n",
        "        indices = [word_to_ix[word] for word in word_list if word in word_to_ix]\n",
        "        t = torch.tensor(indices)\n",
        "        if device is not None:\n",
        "            t = t.to(device)  # Transfer tensor to the specified device\n",
        "        index_batch.append(t)\n",
        "\n",
        "    # Pad tensors to have the same length\n",
        "    return pad_tensors(index_batch)\n",
        "\n",
        "# Function to convert a tensor of indices to a list of sequences of words\n",
        "def tensor_to_words(tensor):\n",
        "    index_batch = tensor.cpu().numpy().tolist()\n",
        "    res = []\n",
        "    for indices in index_batch:\n",
        "        words = []\n",
        "        for ix in indices:\n",
        "            words.append(ix_to_word[ix].lower())  # Convert index to word\n",
        "            if ix == word_to_ix[\"<end>\"]:\n",
        "                break  # Stop when <end> token is encountered\n",
        "        res.append(\" \".join(words))\n",
        "    return res\n",
        "\n",
        "# Function to pad a list of tensors to the same length\n",
        "def pad_tensors(list_of_tensors):\n",
        "    tensor_count = len(list_of_tensors) if not torch.is_tensor(list_of_tensors) else list_of_tensors.shape[0]\n",
        "    max_dim = max(t.shape[0] for t in list_of_tensors)  # Find the maximum length\n",
        "    res = []\n",
        "    for t in list_of_tensors:\n",
        "        # Create a zero tensor of the desired shape\n",
        "        res_t = torch.zeros(max_dim, *t.shape[1:]).type(t.dtype).to(t.device)\n",
        "        res_t[:t.shape[0]] = t  # Copy the original tensor into the padded tensor\n",
        "        res.append(res_t)\n",
        "\n",
        "    # Concatenate tensors along a new dimension\n",
        "    res = torch.cat(res)\n",
        "    firstDim = len(list_of_tensors)\n",
        "    secondDim = max_dim\n",
        "\n",
        "    # Reshape the result to have the new dimension first\n",
        "    return res.reshape(firstDim, secondDim, *res.shape[1:])\n",
        "\n",
        "# Define Self-Attention module\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, head_count):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size  # Size of word embeddings\n",
        "        self.head_count = head_count  # Number of attention heads\n",
        "\n",
        "        # Create linear layers for query, key and value projections for each head\n",
        "        self.query_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])\n",
        "        self.key_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])\n",
        "        self.value_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])\n",
        "        self.fc_out = nn.Linear(head_count * embed_size, embed_size)  # Final linear layer to combine head outputs\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        batch_size, token_count = embeddings.shape[:2]\n",
        "        qkvs = torch.zeros(self.head_count, 3, batch_size, token_count, self.embed_size).to(embeddings.device)\n",
        "\n",
        "        # Loop over heads and compute query, key and value projections\n",
        "        for i in range(self.head_count):\n",
        "            qkvs[i, 0] = self.query_layers[i](embeddings)\n",
        "            qkvs[i, 1] = self.key_layers[i](embeddings)\n",
        "            qkvs[i, 2] = self.value_layers[i](embeddings)\n",
        "\n",
        "        # Compute energy terms for each head, batch, and pair of tokens\n",
        "        energy = torch.zeros(self.head_count, batch_size, token_count, token_count).to(embeddings.device)\n",
        "        # Create a mask with false on and below the diagonal, and true above the diagonal\n",
        "        mask = torch.triu(torch.ones((token_count, token_count)), diagonal=1).bool()\n",
        "\n",
        "        for h in range(self.head_count):\n",
        "            for b in range(batch_size):\n",
        "                for i in range(token_count):\n",
        "                    for j in range(token_count):\n",
        "                        energy[h, b, i, j] = torch.dot(qkvs[h, 0, b, i], qkvs[h, 1, b, j])\n",
        "                energy[h, b] = energy[h, b].masked_fill(mask, float('-inf')) # Apply mask\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention = torch.nn.functional.softmax(energy, dim=3)\n",
        "\n",
        "        # Compute weighted sum of values for each head and token\n",
        "        out = torch.zeros(batch_size, token_count, self.head_count, self.embed_size).to(embeddings.device)\n",
        "        for h in range(self.head_count):\n",
        "            for b in range(batch_size):\n",
        "                for i in range(token_count):\n",
        "                    for j in range(token_count):\n",
        "                        out[b, i, h] += (attention[h, b, i, j] * qkvs[h, 2, b, j])\n",
        "\n",
        "        # Reshape and pass through final linear layer\n",
        "        out = out.reshape(batch_size, token_count, self.head_count * self.embed_size)\n",
        "        return self.fc_out(out)\n",
        "\n",
        "    def masked_attention(self, energy):\n",
        "        # Assume scores has shape (batch_size, max_token_count, embed_size, embed_size)\n",
        "        max_token_count, embed_size, _ = energy.size()\n",
        "\n",
        "        # Create a mask with zeros on and below the diagonal, and negative infinity above the diagonal\n",
        "        mask = torch.triu(torch.ones((max_token_count, max_token_count)), diagonal=1) * float('-inf')\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)  # Add dimensions for batch and embedding size\n",
        "        mask = mask.expand(batch_size, embed_size, -1, -1)  # Expand mask to match batch and embedding size\n",
        "\n",
        "        # Apply the mask to the scores\n",
        "        masked_scores = energy + mask.to(energy.device)\n",
        "\n",
        "        return masked_scores.to(energy.device)\n",
        "\n",
        "# Define Transformer block module\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, head_count):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, head_count)  # Self-attention layer\n",
        "        self.norm1 = nn.LayerNorm(embed_size)  # Layer normalization\n",
        "        self.norm2 = nn.LayerNorm(embed_size)  # Layer normalization\n",
        "\n",
        "        # Feed-forward neural network\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        attention = self.attention(embeddings)\n",
        "\n",
        "        # Apply residual connections and layer normalization\n",
        "        out = self.norm1(attention + embeddings)\n",
        "        out = attention + self.feed_forward(out)\n",
        "        out = self.norm2(out)\n",
        "        return out\n",
        "\n",
        "# Define Transformer module\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, head_count):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_size = embed_size  # Size of word embeddings\n",
        "        self.vocab_size = vocab_size  # Size of vocabulary\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)  # Embedding layer\n",
        "\n",
        "        # List of transformer blocks\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, head_count) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)  # Final linear layer to produce logits\n",
        "\n",
        "    def forward(self, input_tokens, mask=None):\n",
        "        batch_size, token_count = input_tokens.shape[:2]\n",
        "        out = self.word_embedding(input_tokens)  # Obtain word embeddings\n",
        "\n",
        "        # Compute position encodings and add to word embeddings\n",
        "        positions = torch.arange(0, token_count).expand(batch_size, token_count).to(input_tokens.device)\n",
        "        position_encoding = self.position_encoding(positions, self.embed_size)\n",
        "        out += position_encoding.reshape(out.shape)\n",
        "\n",
        "        # Pass through each transformer block\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "\n",
        "        # Produce logits for the final token in each sequence\n",
        "        out = self.fc_out(out[:, -1, :].reshape(batch_size, self.embed_size)).reshape(batch_size, self.vocab_size)\n",
        "        return torch.nn.functional.softmax(out, dim=1)  # Apply softmax to obtain probabilities\n",
        "\n",
        "    def position_encoding(self, positions, embed_size):\n",
        "        # Compute position encoding for each position and dimension\n",
        "        angle_rads = self.get_angles(\n",
        "            positions.unsqueeze(2).float(),\n",
        "            torch.arange(embed_size)[None, None, :].float().to(positions.device),\n",
        "            embed_size\n",
        "        )\n",
        "        sines = torch.sin(angle_rads[:, :, 0::2])  # Compute sine of angle for even dimensions\n",
        "        cosines = torch.cos(angle_rads[:, :, 1::2])  # Compute cosine of angle for odd dimensions\n",
        "        pos_encoding = torch.cat([sines, cosines], dim=-1)  # Concatenate sine and cosine values\n",
        "        pos_encoding = pos_encoding[None, ...]\n",
        "        return pos_encoding\n",
        "\n",
        "    def get_angles(self, pos, i, embed_size):\n",
        "        # Compute angle rate for each position and dimension\n",
        "        angle_rates = 1 / torch.pow(10000, (2 * (i//2)) / embed_size)\n",
        "        return pos * angle_rates\n",
        "\n",
        "# Function to train the model recursively over each sequence and token\n",
        "def train_recursive(model, data, targets, optimizer, criterion):\n",
        "    model.train()  # Set model to training mode\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    total_loss = 0  # Initialize total loss\n",
        "    batch_size, token_count, token_count_out = data.shape[0], data.shape[1], targets.shape[1]\n",
        "\n",
        "    # Loop over sequences in the batch\n",
        "    for b in range(batch_size):\n",
        "        end_encountered = False\n",
        "        cur_count = 0\n",
        "        # Loop over tokens in the sequence\n",
        "        while not end_encountered:\n",
        "            target_vector = torch.zeros(model.vocab_size).to(data.device)  # Initialize target vector\n",
        "\n",
        "            if cur_count != token_count_out:\n",
        "                expected_next_token_idx = targets[b, cur_count]  # Get index of expected next token\n",
        "                target_vector[expected_next_token_idx] = 1  # Set the corresponding element of the target vector to 1\n",
        "\n",
        "            # Concatenate current input and output tokens and pass through model\n",
        "            if cur_count > 0:\n",
        "                model_input = data[b].reshape(token_count).to(data.device)\n",
        "                part_of_output = targets[b, :cur_count].to(data.device)\n",
        "                model_input = torch.cat((model_input, part_of_output))\n",
        "            else:\n",
        "                model_input = data[b]\n",
        "            out = model(model_input.reshape(1, token_count + cur_count))\n",
        "\n",
        "            # Compute loss and accumulate total loss\n",
        "            loss = criterion(out, target_vector.reshape(out.shape))\n",
        "            total_loss += loss\n",
        "            cur_count += 1\n",
        "\n",
        "            # Stop when the end of the sequence is reached\n",
        "            if cur_count > token_count_out:\n",
        "                end_encountered = True\n",
        "\n",
        "    # Backpropagate gradients and update model parameters\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    return total_loss.item() / batch_size\n",
        "\n",
        "# Function to perform inference recursively for each sequence in a batch\n",
        "def infer_recursive(model, input_vectors, max_output_token_count=10):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    outputs = []\n",
        "\n",
        "    # Loop over sequences in the batch\n",
        "    for i in range(input_vectors.shape[0]):\n",
        "        print(f\"Infering sequence {i}\")\n",
        "        input_vector = input_vectors[i].reshape(1, input_vectors.shape[1])\n",
        "        predicted_sequence = []\n",
        "        wc = 0  # Initialize word count\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            while True:\n",
        "                output = model(input_vector)  # Pass current input through model\n",
        "                predicted_index = output[0, :].argmax().item()  # Get index of predicted token\n",
        "                predicted_sequence.append(predicted_index)  # Append predicted index to sequence\n",
        "                # Stop when <end> token is predicted or the maximum output length is reached\n",
        "                if predicted_index == word_to_ix['<end>'] or wc > max_output_token_count:\n",
        "                    break\n",
        "                # Append predicted token to input and increment word count\n",
        "                input_vector = torch.cat([input_vector, torch.tensor([[predicted_index]])], dim=1)\n",
        "                wc += 1\n",
        "        outputs.append(torch.tensor(predicted_sequence))  # Append predicted sequence to outputs\n",
        "    outputs = pad_tensors(outputs)  # Pad predicted sequences to the same length\n",
        "    return outputs\n",
        "\n",
        "# Function to demonstrate training and inference\n",
        "def example_training_and_inference():\n",
        "    # Get model hyperparameters from vocabulary size\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embed_size = 512\n",
        "    num_layers = 4\n",
        "    heads = 3\n",
        "\n",
        "    # Create model, optimizer, and loss function\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = Transformer(vocab_size, embed_size, num_layers, heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Convert training data to tensors\n",
        "    data = words_to_tensor(data_words, device=device)\n",
        "    targets = words_to_tensor(target_words, device=device)\n",
        "\n",
        "    # Train model for 55 epochs\n",
        "    for epoch in range(55):\n",
        "        avg_loss = train_recursive(model, data, targets, optimizer, criterion)\n",
        "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Perform inference on training data\n",
        "    input_vector = words_to_tensor(data_words, device=device)\n",
        "    predicted_vector = infer_recursive(model, input_vector)\n",
        "    predicted_words = tensor_to_words(predicted_vector)\n",
        "\n",
        "    # Print training data and model output\n",
        "    print(\"\\n\\n\\n\")\n",
        "    print(\"Training Data:\")\n",
        "    pprint.pprint(training_data)\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"Model Inference:\")\n",
        "    result_data = {data_words[k]: predicted_words[k] for k in range(len(predicted_words))}\n",
        "    pprint.pprint(result_data)\n",
        "\n",
        "# Main function to call the demonstration function\n",
        "if __name__ == \"__main__\":\n",
        "    # Get training data and vocabulary\n",
        "    training_data, data_words, target_words, vocabulary_words, word_to_ix, ix_to_word = get_data_and_vocab()\n",
        "    # Run the example training and inference function\n",
        "    example_training_and_inference()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlLcr-ChruJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}