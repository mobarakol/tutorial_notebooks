{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdM1mJVqZzCnqcG2m40pCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAM_LoRA_DoRA_AdaLoRA_VeLoRA_MoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!wget wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfghcnj0M6QY",
        "outputId": "e30d02c8-a16a-4991-a413-adbf3b1b99cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2025-06-28 15:43:03--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2025-06-28 15:43:03--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.111, 13.35.37.123, 13.35.37.90, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375042383 (358M) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_b_01ec64.pth’\n",
            "\n",
            "sam_vit_b_01ec64.pt 100%[===================>] 357.67M  32.9MB/s    in 5.2s    \n",
            "\n",
            "2025-06-28 15:43:08 (68.9 MB/s) - ‘sam_vit_b_01ec64.pth’ saved [375042383/375042383]\n",
            "\n",
            "FINISHED --2025-06-28 15:43:08--\n",
            "Total wall clock time: 5.3s\n",
            "Downloaded: 1 files, 358M in 5.2s (68.9 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identify Target Modules for Peft in SAM"
      ],
      "metadata": {
        "id": "W-WG1yutN-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import sam_model_registry\n",
        "\n",
        "sam = sam_model_registry[\"vit_b\"](\n",
        "    checkpoint=\"sam_vit_b_01ec64.pth\")\n",
        "sam_encoder = sam.image_encoder\n",
        "\n",
        "for name, module in sam_encoder.named_modules():\n",
        "    print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-EVuLmSN9pK",
        "outputId": "52500835-968f-4df5-f34e-a79ee6bb5d4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "patch_embed\n",
            "patch_embed.proj\n",
            "blocks\n",
            "blocks.0\n",
            "blocks.0.norm1\n",
            "blocks.0.attn\n",
            "blocks.0.attn.qkv\n",
            "blocks.0.attn.proj\n",
            "blocks.0.norm2\n",
            "blocks.0.mlp\n",
            "blocks.0.mlp.lin1\n",
            "blocks.0.mlp.lin2\n",
            "blocks.0.mlp.act\n",
            "blocks.1\n",
            "blocks.1.norm1\n",
            "blocks.1.attn\n",
            "blocks.1.attn.qkv\n",
            "blocks.1.attn.proj\n",
            "blocks.1.norm2\n",
            "blocks.1.mlp\n",
            "blocks.1.mlp.lin1\n",
            "blocks.1.mlp.lin2\n",
            "blocks.1.mlp.act\n",
            "blocks.2\n",
            "blocks.2.norm1\n",
            "blocks.2.attn\n",
            "blocks.2.attn.qkv\n",
            "blocks.2.attn.proj\n",
            "blocks.2.norm2\n",
            "blocks.2.mlp\n",
            "blocks.2.mlp.lin1\n",
            "blocks.2.mlp.lin2\n",
            "blocks.2.mlp.act\n",
            "blocks.3\n",
            "blocks.3.norm1\n",
            "blocks.3.attn\n",
            "blocks.3.attn.qkv\n",
            "blocks.3.attn.proj\n",
            "blocks.3.norm2\n",
            "blocks.3.mlp\n",
            "blocks.3.mlp.lin1\n",
            "blocks.3.mlp.lin2\n",
            "blocks.3.mlp.act\n",
            "blocks.4\n",
            "blocks.4.norm1\n",
            "blocks.4.attn\n",
            "blocks.4.attn.qkv\n",
            "blocks.4.attn.proj\n",
            "blocks.4.norm2\n",
            "blocks.4.mlp\n",
            "blocks.4.mlp.lin1\n",
            "blocks.4.mlp.lin2\n",
            "blocks.4.mlp.act\n",
            "blocks.5\n",
            "blocks.5.norm1\n",
            "blocks.5.attn\n",
            "blocks.5.attn.qkv\n",
            "blocks.5.attn.proj\n",
            "blocks.5.norm2\n",
            "blocks.5.mlp\n",
            "blocks.5.mlp.lin1\n",
            "blocks.5.mlp.lin2\n",
            "blocks.5.mlp.act\n",
            "blocks.6\n",
            "blocks.6.norm1\n",
            "blocks.6.attn\n",
            "blocks.6.attn.qkv\n",
            "blocks.6.attn.proj\n",
            "blocks.6.norm2\n",
            "blocks.6.mlp\n",
            "blocks.6.mlp.lin1\n",
            "blocks.6.mlp.lin2\n",
            "blocks.6.mlp.act\n",
            "blocks.7\n",
            "blocks.7.norm1\n",
            "blocks.7.attn\n",
            "blocks.7.attn.qkv\n",
            "blocks.7.attn.proj\n",
            "blocks.7.norm2\n",
            "blocks.7.mlp\n",
            "blocks.7.mlp.lin1\n",
            "blocks.7.mlp.lin2\n",
            "blocks.7.mlp.act\n",
            "blocks.8\n",
            "blocks.8.norm1\n",
            "blocks.8.attn\n",
            "blocks.8.attn.qkv\n",
            "blocks.8.attn.proj\n",
            "blocks.8.norm2\n",
            "blocks.8.mlp\n",
            "blocks.8.mlp.lin1\n",
            "blocks.8.mlp.lin2\n",
            "blocks.8.mlp.act\n",
            "blocks.9\n",
            "blocks.9.norm1\n",
            "blocks.9.attn\n",
            "blocks.9.attn.qkv\n",
            "blocks.9.attn.proj\n",
            "blocks.9.norm2\n",
            "blocks.9.mlp\n",
            "blocks.9.mlp.lin1\n",
            "blocks.9.mlp.lin2\n",
            "blocks.9.mlp.act\n",
            "blocks.10\n",
            "blocks.10.norm1\n",
            "blocks.10.attn\n",
            "blocks.10.attn.qkv\n",
            "blocks.10.attn.proj\n",
            "blocks.10.norm2\n",
            "blocks.10.mlp\n",
            "blocks.10.mlp.lin1\n",
            "blocks.10.mlp.lin2\n",
            "blocks.10.mlp.act\n",
            "blocks.11\n",
            "blocks.11.norm1\n",
            "blocks.11.attn\n",
            "blocks.11.attn.qkv\n",
            "blocks.11.attn.proj\n",
            "blocks.11.norm2\n",
            "blocks.11.mlp\n",
            "blocks.11.mlp.lin1\n",
            "blocks.11.mlp.lin2\n",
            "blocks.11.mlp.act\n",
            "neck\n",
            "neck.0\n",
            "neck.1\n",
            "neck.2\n",
            "neck.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Common Target Modules are:\n",
        "\n",
        "attn.qkv\n",
        "\n",
        "attn.proj\n",
        "\n",
        "mlp.fc1\n",
        "\n",
        "mlp.fc2"
      ],
      "metadata": {
        "id": "7KyoJr-2ORj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply LoRA on SAM"
      ],
      "metadata": {
        "id": "BhE6CdjpOlY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKAlhzmYMQKB",
        "outputId": "992a5e4c-336f-4d3f-fb5a-76562c8eaf88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 442,368 || all params: 90,113,280 || trainable%: 0.4909\n"
          ]
        }
      ],
      "source": [
        "from segment_anything import sam_model_registry\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "sam = sam_model_registry[\"vit_b\"](\n",
        "    checkpoint=\"sam_vit_b_01ec64.pth\")\n",
        "sam_encoder = sam.image_encoder\n",
        "\n",
        "# LoRA configuration for SAM\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    #target_modules=[\"qkv\", \"proj\"],  # Adjust to your SAM model inspection\n",
        "    target_modules=[\"attn.qkv\", \"attn.proj\", \"mlp.fc1\",\"mlp.fc2\"], # SAM layer to apply PEFT\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"FEATURE_EXTRACTION\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to SAM encoder\n",
        "sam_encoder_lora = get_peft_model(sam_encoder, lora_config)\n",
        "\n",
        "# Verify LoRA parameters\n",
        "sam_encoder_lora.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply DoRA on SAM"
      ],
      "metadata": {
        "id": "XxPeQXU3Oucd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dora_config = LoraConfig(\n",
        "    r=8,  # Rank of adaptation\n",
        "    # Target modules specific to vision transformers\n",
        "    # target_modules=[\"qkv\", \"proj\", \"fc1\", \"fc2\"],  # Adjust to your SAM model inspection\n",
        "    target_modules=[\"attn.qkv\", \"attn.proj\", \"mlp.fc1\",\"mlp.fc2\"], # SAM layer to apply PEFT\n",
        "    lora_dropout=0.1,\n",
        "    use_dora=True,  # Enable DoRA (Weight-Decomposed Low-Rank Adaptation)\n",
        "    bias=\"none\",  # Don't adapt bias terms\n",
        "    task_type=\"FEATURE_EXTRACTION\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to SAM encoder\n",
        "sam_encoder_dora = get_peft_model(sam_encoder, dora_config)\n",
        "\n",
        "# Verify LoRA parameters\n",
        "sam_encoder_dora.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCTEd1kmMyhl",
        "outputId": "8e466934-9692-485f-a88f-c63c3061177c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,635,360 || all params: 91,306,296 || trainable%: 1.7911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply AdaLoRA on SAM"
      ],
      "metadata": {
        "id": "7kg_-A-tQIFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AdaLoraConfig, TaskType\n",
        "\n",
        "# Assume you have a fixed number of epochs and batches\n",
        "num_epochs = 60\n",
        "batches_per_epoch = 100 #len(dataloader)  # your DataLoader\n",
        "total_steps = num_epochs * batches_per_epoch\n",
        "\n",
        "# Configure AdaLoRA adapters\n",
        "adalora_config = AdaLoraConfig(\n",
        "    r=8,  # Low-rank matrix rank\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate\n",
        "    # target_modules=[\"attn.qkv\", \"attn.proj\"], # SAM layer to apply PEFT\n",
        "    target_modules=[\"attn.qkv\", \"attn.proj\", \"mlp.fc1\",\"mlp.fc2\"], # SAM layer to apply PEFT\n",
        "    beta1=0.85,  # Adaptive importance weighting factor\n",
        "    beta2=0.85,  # Further control factor for importance weighting\n",
        "    orth_reg_weight=0.5,  # Orthogonality regularization weight\n",
        "    total_step=total_steps,  # Required!\n",
        "    task_type=\"FEATURE_EXTRACTION\"\n",
        ")\n",
        "\n",
        "# Apply AdaLoRA to SAM encoder\n",
        "sam_encoder_adalora = get_peft_model(sam_encoder, adalora_config)\n",
        "\n",
        "# Verify LoRA parameters\n",
        "sam_encoder_adalora.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr5_Mpr_QMmn",
        "outputId": "a7dea45b-3f6e-4c8e-fc2f-d69f109e4739"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,156,128 || all params: 90,827,064 || trainable%: 1.2729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply VeLoRA on SAM"
      ],
      "metadata": {
        "id": "_0k0ai3eQIND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure VeLoRA adapters (Approximating VeLoRA with Variable Low-Rank Updates)\n",
        "velora_config = LoraConfig(\n",
        "    r=16,  # Initial low-rank matrix rank\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate\n",
        "    target_modules=[\"attn.qkv\", \"attn.proj\"], # SAM layer to apply PEFT\n",
        "    rank_pattern={\"attn.qkv\": 16, \"attn.proj\": 8},  # Simulating variable rank adjustment\n",
        "    task_type=\"FEATURE_EXTRACTION\"\n",
        ")\n",
        "\n",
        "# Apply VeLoRA to SAM encoder\n",
        "sam_encoder_velora = get_peft_model(sam_encoder, velora_config)\n",
        "\n",
        "# Verify LoRA parameters\n",
        "sam_encoder_velora.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shUrtIABQNi3",
        "outputId": "46844ba4-f4a9-4ec6-878a-2b321530e86c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 737,280 || all params: 90,408,192 || trainable%: 0.8155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This will replace original hf peft lib. You must use a dedicated vir env for MoRA.\n",
        "#Apply MoRA on SAM\n",
        "installing peft MoRA"
      ],
      "metadata": {
        "id": "fPUW36ypQIP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kongds/MoRA.git\n",
        "%cd MoRA\n",
        "!pip -q install -e ./peft-mora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee8IvSDCaNti",
        "outputId": "0e793fc7-476e-49f0-9012-3fe8306d69b7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MoRA'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 176 (delta 57), reused 136 (delta 33), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (176/176), 257.55 KiB | 16.10 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/content/MoRA\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Must restart the session in colab notebook: Runtime->Restart Session"
      ],
      "metadata": {
        "id": "D33b2mO_bUDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "sam = sam_model_registry[\"vit_b\"](\n",
        "    checkpoint=\"sam_vit_b_01ec64.pth\")\n",
        "sam_encoder = sam.image_encoder\n",
        "\n",
        "\n",
        "mora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"attn.qkv\", \"attn.proj\", \"mlp.fc1\", \"mlp.fc2\"],  # linear layers only!\n",
        "    task_type=\"FEATURE_EXTRACTION\",\n",
        "    use_mora=True,\n",
        "    mora_type=6,\n",
        ")\n",
        "\n",
        "# Apply MoRA to SAM encoder\n",
        "sam_encoder_mora = get_peft_model(sam_encoder, mora_config)\n",
        "\n",
        "# Verify LoRA parameters\n",
        "sam_encoder_mora.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqVqbx-tQOSH",
        "outputId": "c0dd5046-4390-452b-e466-85ed51564b71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 437,232 || all params: 90,108,144 || trainable%: 0.48523028062813056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUhn9C6hPMDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}