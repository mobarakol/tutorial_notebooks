{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Explaining_Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "src: https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_captioning/Image%20Captioning%20using%20Open%20Source.html<br>\n",
        "erc2: https://content.iospress.com/articles/ai-communications/aic210172#:~:text=Image%20Captioning%20is%20the%20task,to%20help%20visually%20impaired%20people."
      ],
      "metadata": {
        "id": "x1BbZLMXp98P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ruotianluo/ImageCaptioning.pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67C8ebMkg2eq",
        "outputId": "dde7cee7-dc07-417a-c324-fc91fe06c0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ImageCaptioning.pytorch'...\n",
            "remote: Enumerating objects: 2268, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 2268 (delta 0), reused 4 (delta 0), pack-reused 2264\u001b[K\n",
            "Receiving objects: 100% (2268/2268), 1.37 MiB | 14.93 MiB/s, done.\n",
            "Resolving deltas: 100% (1601/1601), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ImageCaptioning.pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52-NxoXfhIw-",
        "outputId": "38d59d7a-224b-481a-fd4f-d507456bd354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ImageCaptioning.pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -s -o model-best.pth 'https://drive.google.com/drive/folders/1OsB_jLDorJnzKz6xsOfk1n493P3hwOP0'\n",
        "!curl -L -s -o infos_fc_nsc-best.pkl 'https://drive.google.com/drive/folders/1OsB_jLDorJnzKz6xsOfk1n493P3hwOP0'\n",
        "!curl -L -s -o resnet101 'https://drive.google.com/drive/folders/0B7fNdx_jAqhtbVYzOURMdDNHSGM'\n",
        "!mv resnet101 data/imagenet_weights "
      ],
      "metadata": {
        "id": "IQCgyAEOhLah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install shap yacs lmdbdict transformers\n",
        "!pip -q install git+https://github.com/ruotianluo/meshed-memory-transformer.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcD_Q7dBhgr3",
        "outputId": "18b2fac8-4c03-45d0-f077-a44d3330baa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ruotianluo/meshed-memory-transformer.git\n",
            "  Cloning https://github.com/ruotianluo/meshed-memory-transformer.git to /tmp/pip-req-build-hzwguqik\n",
            "  Running command git clone -q https://github.com/ruotianluo/meshed-memory-transformer.git /tmp/pip-req-build-hzwguqik\n",
            "Building wheels for collected packages: meshed-memory-transformer\n",
            "  Building wheel for meshed-memory-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for meshed-memory-transformer: filename=meshed_memory_transformer-0.0.1-py3-none-any.whl size=39551 sha256=ee5bc636e364409df9e29bed6dc2df233992386bf52dc14131a0ff30a865b635\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7bwcj143/wheels/89/75/b0/1210778401d564ce5daceae0bed8ee6089fe7b65c7224d7e78\n",
            "Successfully built meshed-memory-transformer\n",
            "Installing collected packages: meshed-memory-transformer\n",
            "Successfully installed meshed-memory-transformer-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA1A8I5KgbC5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shap\n",
        "from shap.utils.image import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change PREFIX to have absolute path of cloned directory of ImageCaptioning.pytorch\n",
        "PREFIX = r\"/content/ImageCaptioning.pytorch\"\n",
        "os.chdir(PREFIX)\n",
        "\n",
        "# directory of images to be explained\n",
        "DIR = './test_images/'\n",
        "# creates or empties directory if it already exists\n",
        "make_dir(DIR)\n",
        "add_sample_images(DIR)\n",
        "\n",
        "# directory for saving masked images\n",
        "DIR_MASKED = './masked_images/'"
      ],
      "metadata": {
        "id": "zalMcvbnghD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import captioning\n",
        "import captioning.models as models\n",
        "import captioning.utils.eval_utils as eval_utils\n",
        "import captioning.utils.misc as utils\n",
        "import captioning.modules.losses as losses\n",
        "from captioning.data.dataloader import *\n",
        "from captioning.data.dataloaderraw import *\n",
        "import gc\n",
        "import sys\n",
        "import torch\n",
        "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
        "\n",
        "# to suppress verbose output from open source model\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr"
      ],
      "metadata": {
        "id": "-aeqtQREj8AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningPyTorchModel:\n",
        "    \"\"\"\n",
        "    Wrapper class to get image captions using Resnet model from setup above.\n",
        "    Note: This class is being used instead of tools/eval.py to get predictions (captions).\n",
        "    To get more context for this class, please refer to tools/eval.py file.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, infos_path, cnn_model = \"resnet101\", device = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initializing the class by loading torch model and vocabulary at path given and using Resnet weights stored in data/imagenet_weights.\n",
        "        This is done to speeden the process of getting image captions and avoid loading the model every time captions are needed.\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_path  : pre-trained model path\n",
        "        infos_path  : pre-trained infos (vocab) path\n",
        "        cnn_model   : resnet model weights to use; options: \"resnet101\" (default), \"resnet152\"\n",
        "        device      : \"cpu\" or \"cuda\" (default)\n",
        "        \"\"\"\n",
        "\n",
        "        # load infos\n",
        "        with open(infos_path, 'rb') as f:\n",
        "            print(f, infos_path)\n",
        "            infos = utils.pickle_load(f)\n",
        "        opt = infos['opt']\n",
        "\n",
        "        # setup the model\n",
        "        opt.model = model_path\n",
        "        opt.cnn_model = cnn_model\n",
        "        opt.device = device\n",
        "        opt.vocab = infos['vocab'] # ix -> word mapping\n",
        "        model = models.setup(opt)\n",
        "        del infos\n",
        "        del opt.vocab\n",
        "        model.load_state_dict(torch.load(opt.model, map_location='cpu'))\n",
        "        model.to(opt.device)\n",
        "        model.eval()\n",
        "        crit = losses.LanguageModelCriterion()\n",
        "\n",
        "        # setup class variables for call function\n",
        "        self.opt = opt\n",
        "        self.model = model\n",
        "        self.crit = crit\n",
        "        self.infos_path = infos_path\n",
        "\n",
        "        # free memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    def __call__(self, image_folder, batch_size):\n",
        "        \"\"\"\n",
        "        Function to get captions for images placed in image_folder.\n",
        "        Parameters\n",
        "        ----------\n",
        "        image_folder: folder of images for which captions are needed\n",
        "        batch_size  : number of images to be evaluated at once\n",
        "        Output\n",
        "        -------\n",
        "        captions    : list of captions for images in image_folder (will return a string if there is only one image in folder)\n",
        "        \"\"\"\n",
        "\n",
        "        # setting eval options\n",
        "        opt = self.opt\n",
        "        opt.batch_size = batch_size\n",
        "        opt.image_folder = image_folder\n",
        "        opt.coco_json = \"\"\n",
        "        opt.dataset = opt.input_json\n",
        "        opt.verbose_loss = 0\n",
        "        opt.verbose = False\n",
        "        opt.dump_path = 0\n",
        "        opt.dump_images = 0\n",
        "        opt.num_images = -1\n",
        "        opt.language_eval = 0\n",
        "\n",
        "        # loading vocab\n",
        "        with open(self.infos_path, 'rb') as f:\n",
        "            infos = utils.pickle_load(f)\n",
        "        opt.vocab = infos['vocab']\n",
        "\n",
        "        # creating Data Loader instance to load images\n",
        "        if len(opt.image_folder) == 0:\n",
        "            loader = DataLoader(opt)\n",
        "        else:\n",
        "            loader = DataLoaderRaw({'folder_path': opt.image_folder,\n",
        "                                    'coco_json': opt.coco_json,\n",
        "                                    'batch_size': opt.batch_size,\n",
        "                                    'cnn_model': opt.cnn_model})\n",
        "\n",
        "        # when evaluating using provided pretrained model, vocab may be different from what is in cocotalk.json.\n",
        "        # hence, setting vocab from infos file.\n",
        "        loader.dataset.ix_to_word = opt.vocab\n",
        "        del infos\n",
        "        del opt.vocab\n",
        "\n",
        "        # getting caption predictions\n",
        "        _, split_predictions, _ = eval_utils.eval_split(self.model, self.crit, loader, vars(opt))\n",
        "        captions = []\n",
        "        for line in split_predictions:\n",
        "            captions.append(line['caption'])\n",
        "\n",
        "        # free memory\n",
        "        del loader\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return captions if len(captions) > 1 else captions[0]\n",
        "\n",
        "\n",
        "# create instance of ImageCaptioningPyTorchModel\n",
        "osmodel = ImageCaptioningPyTorchModel(model_path = \"model-best.pth\",\n",
        "                        infos_path = \"infos_fc_nsc-best.pkl\",\n",
        "                        cnn_model = \"resnet101\",\n",
        "                        device = \"cpu\")\n",
        "\n",
        "# create function to get caption using model created above\n",
        "def get_caption(model, image_folder, batch_size):\n",
        "    return model(image_folder, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "esQ8F1EskLLp",
        "outputId": "b3e4dce7-b863-432b-a08c-e507cd2ea8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_io.BufferedReader name='infos_fc_nsc-best.pkl'> infos_fc_nsc-best.pkl\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1a63104c0075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# create instance of ImageCaptioningPyTorchModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m osmodel = ImageCaptioningPyTorchModel(model_path = \"model-best.pth\",\n\u001b[0m\u001b[1;32m    111\u001b[0m                         \u001b[0minfos_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infos_fc_nsc-best.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"resnet101\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1a63104c0075>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, infos_path, cnn_model, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfos_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ImageCaptioning.pytorch/captioning/utils/misc.py\u001b[0m in \u001b[0;36mpickle_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# return cPickle.load(f, encoding='latin-1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '<'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3X9uukzik19-",
        "outputId": "0422bc08-e5f5-4200-a506-7a4a1c9dac1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "data = pickle.load(\"infos_fc_nsc-best.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "wcUovL8amMEL",
        "outputId": "732d8903-ea94-461c-a68c-28c9d6b6dceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-55d1cd859e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"infos_fc_nsc-best.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: file must have 'read' and 'readline' attributes"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"infos_fc_nsc-best.pkl\", 'rb') as pickle_file:\n",
        "    pickle_file.encoding = 'latin-1'\n",
        "    content = pickle.load(pickle_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "SjwLXQjDogjq",
        "outputId": "533b77a9-1908-4a27-e67c-43e67da5c6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f6594efaa2d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"infos_fc_nsc-best.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpickle_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '<'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mnist.pkl', 'rb') as f: \n",
        "    #file = pickle._Unpickler(f)\n",
        "    f.encoding = 'latin1' \n",
        "    file = pickle.load(f) \n",
        "    #train_set, valid_set, test_set = pickle.load(file), \n",
        "    file.close()"
      ],
      "metadata": {
        "id": "38nf3KbtowPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}