{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82bea80b4d6b48f18718b8e6d26cff33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f3b1e47026b452cb50417955419dde2",
              "IPY_MODEL_b5e14139e9c2410d9ce7df8242ca070a",
              "IPY_MODEL_0c583fa3ed454c79ab7567234161ff81"
            ],
            "layout": "IPY_MODEL_0588b2cc2f234e9788aa13b8587e6a3d"
          }
        },
        "3f3b1e47026b452cb50417955419dde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ea31e716e34baa84ed04887f97d60d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2ed3187c8783477a8a67ead1f195bd7d",
            "value": "Map:‚Äá100%"
          }
        },
        "b5e14139e9c2410d9ce7df8242ca070a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0968d2566623481c8770cc944716ac72",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8d2191df19f48fb9451189ec30f25e9",
            "value": 5
          }
        },
        "0c583fa3ed454c79ab7567234161ff81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2b4e7556f79438782d4af9e747b3fbc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e598b26f7e9a41e2bd24a1981d7a827d",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá139.37‚Äáexamples/s]"
          }
        },
        "0588b2cc2f234e9788aa13b8587e6a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ea31e716e34baa84ed04887f97d60d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ed3187c8783477a8a67ead1f195bd7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0968d2566623481c8770cc944716ac72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d2191df19f48fb9451189ec30f25e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2b4e7556f79438782d4af9e747b3fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e598b26f7e9a41e2bd24a1981d7a827d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/LLM_GPT2_QA_Finetune_Instruct_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtglDM63X5tN",
        "outputId": "406ec553-b139-4d17-c12e-271f00f8a72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcYt80sfinYT",
        "outputId": "8d21da1c-5ace-4536-ff24-c4c19e806be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.47.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning:"
      ],
      "metadata": {
        "id": "-Q5XeUiZmuTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instruct"
      ],
      "metadata": {
        "id": "3hIkW1CEt1Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if necessary\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Prepare 5 QA samples for instruction tuning\n",
        "qa_samples = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
        "    {\"question\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
        "    {\"question\": \"What is the largest planet?\", \"answer\": \"The largest planet is Jupiter.\"},\n",
        "    {\"question\": \"Who painted the Mona Lisa?\", \"answer\": \"Leonardo da Vinci painted the Mona Lisa.\"},\n",
        "    {\"question\": \"What is the speed of light?\", \"answer\": \"The speed of light is approximately 299,792 kilometers per second.\"}\n",
        "]\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_data(example):\n",
        "    input_text = f\"Question: {example['question']} Answer:\"\n",
        "    target_text = example[\"answer\"]\n",
        "\n",
        "    # Tokenize inputs and labels\n",
        "    inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=256)\n",
        "    labels = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "    # Set labels to ignore padding tokens\n",
        "    inputs[\"labels\"] = [label if label != tokenizer.pad_token_id else -100 for label in labels[\"input_ids\"]]\n",
        "    return inputs\n",
        "\n",
        "# Convert samples to Hugging Face dataset and preprocess\n",
        "dataset = Dataset.from_list(qa_samples)\n",
        "tokenized_dataset = dataset.map(preprocess_data)\n",
        "\n",
        "# Split dataset into train and eval\n",
        "# split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-qa-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=2,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=20,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=1,\n",
        "    save_total_limit=2,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,  # Enable mixed precision training for faster training\n",
        "    report_to=[],  # Disable W&B or any reporting\n",
        ")\n",
        "\n",
        "# Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./gpt2-qa-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-qa-finetuned\")\n",
        "\n",
        "# Load the fine-tuned model for inference\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-qa-finetuned\").to(\"cuda\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-qa-finetuned\")\n",
        "\n",
        "# Inference function\n",
        "def generate_answer(question):\n",
        "    model.eval()\n",
        "    input_text = f\"Question: {question} Answer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=256).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# Example inference\n",
        "question = \"What is the capital of France?\"\n",
        "answer = generate_answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862,
          "referenced_widgets": [
            "82bea80b4d6b48f18718b8e6d26cff33",
            "3f3b1e47026b452cb50417955419dde2",
            "b5e14139e9c2410d9ce7df8242ca070a",
            "0c583fa3ed454c79ab7567234161ff81",
            "0588b2cc2f234e9788aa13b8587e6a3d",
            "d4ea31e716e34baa84ed04887f97d60d",
            "2ed3187c8783477a8a67ead1f195bd7d",
            "0968d2566623481c8770cc944716ac72",
            "c8d2191df19f48fb9451189ec30f25e9",
            "a2b4e7556f79438782d4af9e747b3fbc",
            "e598b26f7e9a41e2bd24a1981d7a827d"
          ]
        },
        "id": "_kqPDbJgtaH-",
        "outputId": "d62be2e2-4ef8-4fe9-8544-1736c680d7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82bea80b4d6b48f18718b8e6d26cff33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-7-dd796ea81604>:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:22, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>11.723400</td>\n",
              "      <td>11.080130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>9.574200</td>\n",
              "      <td>11.080130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>8.447900</td>\n",
              "      <td>8.626542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>8.393600</td>\n",
              "      <td>7.327123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>8.900800</td>\n",
              "      <td>6.092465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.819800</td>\n",
              "      <td>5.137457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>6.751600</td>\n",
              "      <td>4.043090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.397500</td>\n",
              "      <td>3.167568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>4.653500</td>\n",
              "      <td>2.622004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.771700</td>\n",
              "      <td>2.354168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.463200</td>\n",
              "      <td>2.060469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.617100</td>\n",
              "      <td>1.794709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.923700</td>\n",
              "      <td>1.498363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.316300</td>\n",
              "      <td>1.402665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.346100</td>\n",
              "      <td>1.407305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.193600</td>\n",
              "      <td>1.435134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.511700</td>\n",
              "      <td>1.428093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>4.447100</td>\n",
              "      <td>1.467025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.222600</td>\n",
              "      <td>1.468948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>1.493065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the capital of France?\n",
            "A: . Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference: Greedy Search"
      ],
      "metadata": {
        "id": "GDblYXZv0RPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = \"./gpt2-qa-finetuned\"  # Change to your model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure padding token is set\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def greedy_generate_answer(question, max_length=50):\n",
        "    \"\"\"\n",
        "    Generate an answer using greedy search.\n",
        "\n",
        "    Args:\n",
        "        question (str): The input question.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "\n",
        "    # Prepare input with the instruction-based format\n",
        "    input_text = f\"Question: {question} Answer:\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Initialize generated token IDs with input\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get the model's output logits\n",
        "            outputs = model(input_ids=generated_ids)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Select the token with the highest probability (greedy search)\n",
        "            next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
        "\n",
        "            # Append the new token to the sequence\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "\n",
        "            # Stop generating if the EOS token is reached\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated token IDs to text\n",
        "    answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the capital of France?\"\n",
        "answer = greedy_generate_answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")\n"
      ],
      "metadata": {
        "id": "TL52-SSFwxvk",
        "outputId": "5b8ebdd6-e7fc-475f-f919-9609d20758e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the capital of France?\n",
            "A: . Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference: Beam Search"
      ],
      "metadata": {
        "id": "vp6T8-Tm0d-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def generate_answer(question, model, tokenizer, max_length=50, num_beams=5, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate an answer using beam search for the fine-tuned GPT-2 model.\n",
        "\n",
        "    Args:\n",
        "        question (str): The input question.\n",
        "        model (GPT2LMHeadModel): The fine-tuned model.\n",
        "        tokenizer (GPT2Tokenizer): The tokenizer.\n",
        "        max_length (int): Maximum length of the generated text.\n",
        "        num_beams (int): Number of beams for beam search.\n",
        "        device (str): The device to run the model on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input text\n",
        "    input_text = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Extract input_ids and attention_mask\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Initialize beams\n",
        "    beams = [(input_ids, attention_mask, 0.0)]  # List of (sequence, attention_mask, score)\n",
        "    completed_sequences = []\n",
        "\n",
        "    # Beam search loop\n",
        "    for _ in range(max_length):\n",
        "        new_beams = []\n",
        "\n",
        "        for seq, mask, score in beams:\n",
        "            # Stop expanding if sequence ends with EOS token\n",
        "            if seq[0, -1] == tokenizer.eos_token_id:\n",
        "                completed_sequences.append((seq, score))  # Append sequence and score\n",
        "                continue\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                outputs = model(seq, attention_mask=mask)\n",
        "                logits = outputs.logits[:, -1, :]  # Logits of the last token\n",
        "                probs = F.log_softmax(logits, dim=-1)  # Convert to log probabilities\n",
        "\n",
        "            # Get top-k tokens and their log probabilities\n",
        "            top_k_probs, top_k_tokens = torch.topk(probs, num_beams, dim=-1)\n",
        "\n",
        "            # Expand each beam\n",
        "            for prob, token in zip(top_k_probs[0], top_k_tokens[0]):\n",
        "                new_seq = torch.cat([seq, token.unsqueeze(0).unsqueeze(0)], dim=1)  # Append token\n",
        "                new_mask = torch.cat([mask, torch.ones((1, 1), device=device)], dim=1)  # Update attention mask\n",
        "                new_score = score + prob.item()  # Accumulate log probability as float\n",
        "                new_beams.append((new_seq, new_mask, new_score))\n",
        "\n",
        "        # Sort new beams by score and keep top-k\n",
        "        new_beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:num_beams]\n",
        "        beams = new_beams\n",
        "\n",
        "        # Break if all beams end with EOS token\n",
        "        if all(seq[0, -1] == tokenizer.eos_token_id for seq, _, _ in beams):\n",
        "            break\n",
        "\n",
        "    # Add remaining beams to completed sequences\n",
        "    completed_sequences.extend([(seq, float(score)) for seq, _, score in beams])\n",
        "\n",
        "    # Select the sequence with the highest score\n",
        "    best_sequence, _ = max(completed_sequences, key=lambda x: x[1])\n",
        "\n",
        "    # Decode the tokens to text\n",
        "    answer = tokenizer.decode(best_sequence[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-qa-finetuned\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-qa-finetuned\")\n",
        "\n",
        "# Test the generate_answer function\n",
        "question = \"What is the capital of France?\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "st = time.time()\n",
        "answer = generate_answer(question, model, tokenizer, max_length=50, num_beams=5, device=device)\n",
        "en = time.time()\n",
        "print('total time:', en-st)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "WRxYr86AyV_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f67a683-47c0-4602-80e2-60199f774d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total time: 4.90141487121582\n",
            "Question: What is the capital of France?\n",
            "Answer:. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris. Paris\n"
          ]
        }
      ]
    }
  ]
}