{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwYpC1+x4/SyuLZA0XgLKS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAM_DeTR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone DeTR Repo:"
      ],
      "metadata": {
        "id": "3hcvujg0aNwl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP24kjSKIJzo",
        "outputId": "851eace9-0fcc-42bf-cbc2-bf204ee7ccc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'detr'...\n",
            "remote: Enumerating objects: 263, done.\u001b[K\n",
            "remote: Total 263 (delta 0), reused 0 (delta 0), pack-reused 263\u001b[K\n",
            "Receiving objects: 100% (263/263), 12.88 MiB | 22.50 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "/content/detr\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/detr.git\n",
        "%cd detr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downlaod Requirements:"
      ],
      "metadata": {
        "id": "jlVgebiFaRnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# download model\n",
        "url = 'https://drive.google.com/uc?id=1S59Aovb4dNOjsu_YdDLxz5wzpxwzww7p'\n",
        "output = 'eval.zip'\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "!unzip -q eval.zip\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1ycUjt29WI4VjK6grZnQXYdzdEuMhpqNA'\n",
        "gdown.download(url, 'raw_img.jpg', quiet=True)\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "!pip -q install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "id": "LXno5FgKIRsV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace Backbone ResNet with SAM encoder:"
      ],
      "metadata": {
        "id": "grNPoJb8aZ-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/detr\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Backbone modules.\n",
        "\"\"\"\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from typing import Dict, List\n",
        "\n",
        "from util.misc import NestedTensor, is_main_process\n",
        "\n",
        "from models.position_encoding import build_position_encoding\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
        "\n",
        "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
        "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
        "    produce nans.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super(FrozenBatchNorm2d, self).__init__()\n",
        "        self.register_buffer(\"weight\", torch.ones(n))\n",
        "        self.register_buffer(\"bias\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        eps = 1e-5\n",
        "        scale = w * (rv + eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "\n",
        "class BackboneBase(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
        "        super().__init__()\n",
        "        for name, parameter in backbone.named_parameters():\n",
        "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "                parameter.requires_grad_(False)\n",
        "        if return_interm_layers:\n",
        "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
        "        else:\n",
        "            return_layers = {'layer4': \"0\"}\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "        self.num_channels = num_channels\n",
        "        sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b_01ec64.pth\")\n",
        "        sam.eval()\n",
        "        for param in sam.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.sam_encoder = sam.image_encoder\n",
        "\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        print('tensor_list.tensors', tensor_list.tensors.shape)\n",
        "        # xs = self.body(tensor_list.tensors)\n",
        "        xs = OrderedDict()\n",
        "        xs['0'] = self.sam_encoder(tensor_list.tensors)\n",
        "        print('out sam encoder', xs['0'].shape)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for name, x in xs.items():\n",
        "            m = tensor_list.mask\n",
        "            print('m', m.shape)\n",
        "            assert m is not None\n",
        "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            print('mobarak',name, x.shape, mask.shape)\n",
        "            out[name] = NestedTensor(x, mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Backbone(BackboneBase):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str,\n",
        "                 train_backbone: bool,\n",
        "                 return_interm_layers: bool,\n",
        "                 dilation: bool):\n",
        "        backbone = getattr(torchvision.models, name)(\n",
        "            replace_stride_with_dilation=[False, False, dilation],\n",
        "            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n",
        "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)\n",
        "\n",
        "\n",
        "class Joiner(nn.Sequential):\n",
        "    def __init__(self, backbone, position_embedding):\n",
        "        super().__init__(backbone, position_embedding)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self[0](tensor_list)\n",
        "        out: List[NestedTensor] = []\n",
        "        pos = []\n",
        "        for name, x in xs.items():\n",
        "            out.append(x)\n",
        "            # position encoding\n",
        "            pos.append(self[1](x).to(x.tensors.dtype))\n",
        "\n",
        "        return out, pos\n",
        "\n",
        "\n",
        "def build_backbone(args):\n",
        "    position_embedding = build_position_encoding(args)\n",
        "    train_backbone = args.lr_backbone > 0\n",
        "    return_interm_layers = args.masks\n",
        "    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n",
        "    model = Joiner(backbone, position_embedding)\n",
        "    model.num_channels = backbone.num_channels\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KcQJpQdS8xv",
        "outputId": "f06d03d0-fca3-49f4-f2b9-37c8e8e8dad7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/detr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minor Edit on DeTR Builder:"
      ],
      "metadata": {
        "id": "3S0bWp-9aox2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/detr\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from util import box_ops\n",
        "from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n",
        "                       accuracy, get_world_size, interpolate,\n",
        "                       is_dist_avail_and_initialized)\n",
        "\n",
        "# from models.backbone import build_backbone\n",
        "from models.matcher import build_matcher\n",
        "from models.segmentation import (DETRsegm, PostProcessPanoptic, PostProcessSegm,\n",
        "                           dice_loss, sigmoid_focal_loss)\n",
        "from models.transformer import build_transformer\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log:\n",
        "            # TODO this should probably be a separate loss, not hacked in this one here\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(\n",
        "            box_ops.box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_ops.box_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
        "                                mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks.flatten(1)\n",
        "        target_masks = target_masks.view(src_masks.shape)\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "            'masks': self.loss_masks\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
        "        if 'aux_outputs' in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
        "                indices = self.matcher(aux_outputs, targets)\n",
        "                for loss in self.losses:\n",
        "                    if loss == 'masks':\n",
        "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
        "                        continue\n",
        "                    kwargs = {}\n",
        "                    if loss == 'labels':\n",
        "                        # Logging is enabled only for the last layer\n",
        "                        kwargs = {'log': False}\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
        "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "class PostProcess(nn.Module):\n",
        "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, target_sizes):\n",
        "        \"\"\" Perform the computation\n",
        "        Parameters:\n",
        "            outputs: raw outputs of the model\n",
        "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
        "                          For evaluation, this must be the original image size (before any data augmentation)\n",
        "                          For visualization, this should be the image size after data augment, but before padding\n",
        "        \"\"\"\n",
        "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
        "\n",
        "        assert len(out_logits) == len(target_sizes)\n",
        "        assert target_sizes.shape[1] == 2\n",
        "\n",
        "        prob = F.softmax(out_logits, -1)\n",
        "        scores, labels = prob[..., :-1].max(-1)\n",
        "\n",
        "        # convert to [x0, y0, x1, y1] format\n",
        "        boxes = box_ops.box_cxcywh_to_xyxy(out_bbox)\n",
        "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
        "        img_h, img_w = target_sizes.unbind(1)\n",
        "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "        boxes = boxes * scale_fct[:, None, :]\n",
        "\n",
        "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build(args):\n",
        "    # the `num_classes` naming here is somewhat misleading.\n",
        "    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n",
        "    # is the maximum id for a class in your dataset. For example,\n",
        "    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n",
        "    # As another example, for a dataset that has a single class with id 1,\n",
        "    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n",
        "    # For more details on this, check the following discussion\n",
        "    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n",
        "    num_classes = 20 if args.dataset_file != 'coco' else 8\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # for panoptic, we just add a num_classes that is large enough to hold\n",
        "        # max_obj_id + 1, but the exact value doesn't really matter\n",
        "        num_classes = 250\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    backbone = build_backbone(args)\n",
        "\n",
        "    transformer = build_transformer(args)\n",
        "    model = DETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_classes,\n",
        "        num_queries=args.num_queries,\n",
        "        aux_loss=args.aux_loss,\n",
        "    )\n",
        "    if args.masks:\n",
        "        model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
        "    matcher = build_matcher(args)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': args.bbox_loss_coef}\n",
        "    weight_dict['loss_giou'] = args.giou_loss_coef\n",
        "    if args.masks:\n",
        "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
        "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
        "    # TODO this is a hack\n",
        "    if args.aux_loss:\n",
        "        aux_weight_dict = {}\n",
        "        for i in range(args.dec_layers - 1):\n",
        "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
        "        weight_dict.update(aux_weight_dict)\n",
        "\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    if args.masks:\n",
        "        losses += [\"masks\"]\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,\n",
        "                             eos_coef=args.eos_coef, losses=losses)\n",
        "    criterion.to(device)\n",
        "    postprocessors = {'bbox': PostProcess()}\n",
        "    if args.masks:\n",
        "        postprocessors['segm'] = PostProcessSegm()\n",
        "        if args.dataset_file == \"coco_panoptic\":\n",
        "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
        "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
        "\n",
        "    return model, criterion, postprocessors\n",
        "\n",
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "            backbone: torch module of the backbone to be used. See backbone.py\n",
        "            transformer: torch module of the transformer architecture. See transformer.py\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        # self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)#mobarak\n",
        "        self.input_proj = nn.Conv2d(256, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "        self.aux_loss = aux_loss\n",
        "        # sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
        "        # self.encoder_sam = sam.image_encoder\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
        "                                dictionnaries containing the two above keys for each decoder layer.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        # sam_encoder_out = self.encoder_sam(samples.tensors)\n",
        "        # print('sam_encoder_out:', sam_encoder_out.shape)\n",
        "        print('samples:',samples.tensors.shape)\n",
        "        features, pos = self.backbone(samples)\n",
        "        print('features', features[0].tensors.shape)#[1, 2048, 32, 40]\n",
        "        print('pos:', torch.stack(pos).shape)#[1, 1, 256, 32, 40]\n",
        "        src, mask = features[-1].decompose()\n",
        "        print('features src:', src.shape, 'features mask:', mask.shape)#torch.Size([1, 2048, 32, 40]), ([1, 32, 40])\n",
        "        assert mask is not None\n",
        "\n",
        "        print('Transformer inputs', 'self.input_proj(src):', self.input_proj(src).shape)#[1, 256, 32, 40]\n",
        "        print( 'mask',mask.shape, 'embed weight:', self.query_embed.weight.shape)#[1, 32, 40], [100, 256]\n",
        "        print('pos[-1]:', pos[-1].shape)#[1, 256, 32, 40]\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "        print('Transformer outputs', hs.shape)\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        print('outputs_class', outputs_class.shape)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        print('outputs_coord', outputs_coord.shape)\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "        # if self.aux_loss:\n",
        "        #     out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
        "        return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4FSHa_4IVr0",
        "outputId": "e7bf3f24-90d5-403c-ae2d-eef20fea69d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/detr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from main import get_args_parser\n",
        "from PIL import Image\n",
        "# from models import build_model\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize((1024, 1024)),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
        "    args = parser.parse_args([])\n",
        "    args.aux_loss = False\n",
        "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(args.device)\n",
        "    model, criterion, postprocessors = build(args)\n",
        "    model.to(device)\n",
        "    # model.load_state_dict(torch.load('eval_0049.pth', map_location=torch.device('cpu')).state_dict())\n",
        "    # model = torch.load('eval_0049.pth', map_location=torch.device('cpu'))\n",
        "    model.eval();\n",
        "\n",
        "    raw_img = Image.open('raw_img.jpg')\n",
        "    img = transform(raw_img).unsqueeze(0)\n",
        "    outputs = model(img)\n",
        "    print('outputs[pred_logits]:', outputs['pred_logits'].shape)\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D84FiBPIy-a",
        "outputId": "3cf1af80-9a91-4969-fed9-12f19e9dc98d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samples: torch.Size([1, 3, 1024, 1024])\n",
            "tensor_list.tensors torch.Size([1, 3, 1024, 1024])\n",
            "out sam encoder torch.Size([1, 256, 64, 64])\n",
            "m torch.Size([1, 1024, 1024])\n",
            "mobarak 0 torch.Size([1, 256, 64, 64]) torch.Size([1, 64, 64])\n",
            "features torch.Size([1, 256, 64, 64])\n",
            "pos: torch.Size([1, 1, 256, 64, 64])\n",
            "features src: torch.Size([1, 256, 64, 64]) features mask: torch.Size([1, 64, 64])\n",
            "Transformer inputs self.input_proj(src): torch.Size([1, 256, 64, 64])\n",
            "mask torch.Size([1, 64, 64]) embed weight: torch.Size([100, 256])\n",
            "pos[-1]: torch.Size([1, 256, 64, 64])\n",
            "Transformer outputs torch.Size([6, 1, 100, 256])\n",
            "outputs_class torch.Size([6, 1, 100, 9])\n",
            "outputs_coord torch.Size([6, 1, 100, 4])\n",
            "outputs[pred_logits]: torch.Size([1, 100, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debugging output of DeTR with SAM Encoder Vs ResNet:"
      ],
      "metadata": {
        "id": "FQq_NsiUawMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAM Backbone:\n",
        "samples: torch.Size([1, 3, 1024, 1024])\n",
        "tensor_list.tensors torch.Size([1, 3, 1024, 1024])\n",
        "out sam encoder torch.Size([1, 256, 64, 64])\n",
        "m torch.Size([1, 1024, 1024])\n",
        "mobarak 0 torch.Size([1, 256, 64, 64]) torch.Size([1, 64, 64])\n",
        "features torch.Size([1, 256, 64, 64])\n",
        "pos: torch.Size([1, 1, 256, 64, 64])\n",
        "features src: torch.Size([1, 256, 64, 64]) features mask: torch.Size([1, 64, 64])\n",
        "Transformer inputs self.input_proj(src): torch.Size([1, 256, 64, 64])\n",
        "mask torch.Size([1, 64, 64]) embed weight: torch.Size([100, 256])\n",
        "pos[-1]: torch.Size([1, 256, 64, 64])\n",
        "Transformer outputs torch.Size([6, 1, 100, 256])\n",
        "outputs_class torch.Size([6, 1, 100, 9])\n",
        "outputs_coord torch.Size([6, 1, 100, 4])\n",
        "outputs[pred_logits]: torch.Size([1, 100, 9])\n",
        "\n",
        "ResNet Backbone:\n",
        "samples: torch.Size([1, 3, 1024, 1024])\n",
        "tensor_list.tensors torch.Size([1, 3, 1024, 1024])\n",
        "out sam encoder torch.Size([1, 2048, 32, 32])\n",
        "m torch.Size([1, 1024, 1024])\n",
        "mobarak 0 torch.Size([1, 2048, 32, 32]) torch.Size([1, 32, 32])\n",
        "features torch.Size([1, 2048, 32, 32])\n",
        "pos: torch.Size([1, 1, 256, 32, 32])\n",
        "features src: torch.Size([1, 2048, 32, 32]) features mask: torch.Size([1, 32, 32])\n",
        "Transformer inputs self.input_proj(src): torch.Size([1, 256, 32, 32])\n",
        "mask torch.Size([1, 32, 32]) embed weight: torch.Size([100, 256])\n",
        "pos[-1]: torch.Size([1, 256, 32, 32])\n",
        "Transformer outputs torch.Size([6, 1, 100, 256])\n",
        "outputs_class torch.Size([6, 1, 100, 9])\n",
        "outputs_coord torch.Size([6, 1, 100, 4])\n",
        "outputs[pred_logits]: torch.Size([1, 100, 9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ_Jjkw2Jr8S",
        "outputId": "f7b0e229-857c-4426-a4e4-8fd943b1f877"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play with SAM:<br>\n",
        "We used SAM with vit_b which consist 12 multihead attention (MHA) block:<br>\n",
        "model_type = \"vit_b\" = 12 MHA = !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth<br>\n",
        "\n",
        "model_type = \"vit_l\" = 24 MHA = !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth<br>\n",
        "\n",
        "model_type = \"vit_h\" = 32 MHA = !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pt<br>\n",
        "\n",
        "To debug SAM edit the lib:<br>\n",
        "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py<br>\n",
        "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/sam.py"
      ],
      "metadata": {
        "id": "66s6sA-obFwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"/content/detr/sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "# device = \"cuda\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n",
        "sam.eval()\n",
        "#if you want to change the size:\n",
        "# sam, img_embedding_size = sam_model_registry[model_type](image_size=img_size,\n",
        "#                                     num_classes=num_classes,\n",
        "#                                     checkpoint=sam_checkpoint, pixel_mean=[0, 0, 0],\n",
        "#                                     pixel_std=[1, 1, 1])\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "raw_img = Image.open('/content/detr/raw_img.jpg')\n",
        "image = np.array(raw_img)\n",
        "masks = mask_generator.generate(image)\n",
        "print(masks[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U15zI-FaNcvn",
        "outputId": "3c2d677d-5f33-4c1e-ee4c-5783e952f29e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_img = len(masks)\n",
        "num_img = 10\n",
        "fig, ax = plt.subplots(1, num_img, figsize=(1.5*num_img,7), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "ax[0].imshow(image)\n",
        "ax[0].set_title('Input')\n",
        "for i in range(num_img - 1):\n",
        "    ax[i+1].imshow(masks[i]['segmentation'])\n",
        "    ax[i+1].set_title('Pred-{}'.format(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "_AhDM5Bhbam9",
        "outputId": "a25f4f8c-7610-4fda-b2a3-f56fb1e4c683"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAB5CAYAAAB4HevDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABrhklEQVR4nOzdeZxlV1nv/8+z1trDGWrsIfM8gCRhMEAElSA3l1EQZBLlMl9AL4NeES/XAUTR6w8QUARBuaAIKIOKiKDgBUUERCBAICEhczrd6bGmM+y911rP7499upImQVKkO1XVvd68mu6cOufU2vXU6a761rOeJaqqJEmSJEmSJEmSJEmSJMlhZtZ7AUmSJEmSJEmSJEmSJMnRKQVPSZIkSZIkSZIkSZIkyRGRgqckSZIkSZIkSZIkSZLkiEjBU5IkSZIkSZIkSZIkSXJEpOApSZIkSZIkSZIkSZIkOSJS8JQkSZIkSZIkSZIkSZIcESl4SpIkSZIkSZIkSZIkSY6IFDwlSZIkSZIkSZIkSZIkR0QKnpIkSZIkSZIkSZIkSZIjIgVPyVHt9NNP51nPetZ6LyM5AlJtj06prkenVNejU6rr0SnV9eiVant0SnU9Oh1tdd3QwdO73vUuRIT/+I//WO+lMBwOedWrXsWnP/3p9V7KpnKwhgd/lWXJueeey4te9CJuueWW9V7eIaqq4pd/+Zc58cQT6XQ6XHTRRXziE59Y72VtWJultisrK7zyla/kkY98JPPz84gI73rXu9Z7WRvWZqnrF7/4RV70ohdx3nnn0ev1OPXUU3nKU57ClVdeud5L25A2S12/8Y1v8OQnP5kzzzyTbrfL1q1bechDHsJHPvKR9V7ahrRZ6vqdXvOa1yAinH/++eu9lA1ps9T105/+9CHrvO2vz3/+8+u9vA1ps9T2oC9/+cs87nGPY35+nm63y/nnn8/v//7vr/eyNpzNUtdnPetZ3/U1KyLs2LFjvZe4oWyWugJcddVV/NRP/RQnn3wy3W6Xe97znrz61a9mOByu99JWufVewGYxHA75jd/4DQAe+tCHru9iNqFXv/rVnHHGGYzHY/71X/+Vt771rfz93/89l112Gd1ud72XB7R/GX/wgx/k53/+5znnnHN417vexaMf/Wg+9alP8SM/8iPrvbwNa6PXdu/evbz61a/m1FNP5T73uU8Kj++kjV7X3/3d3+Wzn/0sT37yk7n3ve/Nrl27ePOb38wP/uAP8vnPfz59Q/tdbPS6Xn/99SwvL/PMZz6TE088keFwyIc+9CEe97jH8ba3vY3nP//5673EDWmj1/W2brrpJn77t3+bXq+33kvZ8DZLXV/ykpfwgAc84JDbzj777HVazeawGWr7j//4jzz2sY/lfve7H7/2a79Gv9/n6quv5qabblrvpW1YG72uL3jBC7jkkksOuU1VeeELX8jpp5/OSSedtE4r29g2el1vvPFGHvjABzIzM8OLXvQi5ufn+dznPscrX/lKvvSlL/HhD394vZfY0g3sne98pwL6xS9+cb2Xonv27FFAX/nKV673UjaV71bD//k//6cC+t73vvcOH7eysnJY3v9pp52mz3zmM7/n/b7whS8ooK997WtXbxuNRnrWWWfpgx70oMOylqPNZqnteDzWnTt3qqrqF7/4RQX0ne9852FZw9Fos9T1s5/9rFZVdchtV155pRZFoT/zMz9zWNZyNNksdb0j3nu9z33uo/e4xz0Oy1qOJpuxrk996lP1YQ97mF588cV63nnnHZZ1HG02S10/9alPKaAf+MAHDsv7PRZsltouLi7qcccdp094whM0hHBY3vfRbLPU9Y585jOfUUBf85rXHJa1HE02S11f85rXKKCXXXbZIbc/4xnPUED3799/WNZzV23orXbf6VnPehb9fp8dO3bw+Mc/nn6/z7Zt23jZy15GCGH1ftdddx0iwute9zre8IY3cNppp9HpdLj44ou57LLLDnnOhz70oXfYwfSsZz2L008/ffX5tm3bBsBv/MZvrLbbvepVrzpSl3rUe9jDHgbAtddeu1rXq6++mkc/+tFMTU3xMz/zMwDEGHnjG9/IeeedR1mWHHfccbzgBS/gwIEDhzyfqvJbv/Vbq+2FP/ZjP8Y3vvGNO72eD37wg1hrD/lpelmWPPe5z+Vzn/scN95442G46mPDRqttURQcf/zxh+8Cj1Ebra4PfvCDyfP8kNvOOecczjvvPC6//PK7eLXHjo1W1ztireWUU05hYWHhLj3PsWSj1vVf/uVf+OAHP8gb3/jGu3yNx6KNWleA5eVlvPd37QKPYRuttu9973u55ZZbeM1rXoMxhsFgQIzx8F3wMWKj1fWOvPe970VE+Omf/um79DzHko1W16WlJQCOO+64Q24/4YQTMMbc7uvl9bKpgieAEAKPeMQj2LJlC6973eu4+OKLef3rX8/b3/722933z/7sz/j93/99/sf/+B+84hWv4LLLLuNhD3vYmvdkbtu2jbe+9a0APOEJT+Dd73437373u/nJn/zJw3JNx6Krr74agC1btgDgvecRj3gE27dv53Wvex1PfOITgbYl9Jd+6Zf44R/+Yd70pjfx7Gc/m/e85z084hGPoGma1ef79V//dX7t136N+9znPrz2ta/lzDPP5OEPfziDweBOrecrX/kK5557LtPT04fc/sAHPhCASy+99K5e8jFjo9U2OTw2Q11VlVtuuYWtW7fehSs9tmzUug4GA/bu3cvVV1/NG97wBj72sY/xX/7LfzlMV33024h1DSHw4he/mOc973lccMEFh/Fqjx0bsa4Az372s5menqYsS37sx35sQ8xm3Ww2Wm0/+clPMj09zY4dO7jHPe5Bv99nenqan/3Zn2U8Hh/mqz96bbS6fqemaXj/+9/Pgx/84NWGi+R722h1PdhE89znPpdLL72UG2+8kb/8y7/krW99Ky95yUs2ztb29Wq1ujO+s73tmc98pgL66le/+pD73e9+99MLL7xw9b+vvfZaBbTT6ehNN920evvB7VS/8Au/sHrbxRdfrBdffPHt3vczn/lMPe2001b/O221+/4crOEnP/lJ3bNnj9544436F3/xF7ply5bV+hys6//6X//rkMcebP18z3vec8jtH//4xw+5fffu3ZrnuT7mMY/RGOPq/f73//7fCtypFsXzzjtPH/awh93u9m984xsK6B/90R99H1d/dNsstb2ttNXue9uMdT3o3e9+twL6jne84/t6/NFss9X1BS94gQIKqDFGn/SkJ22YVvGNZDPV9c1vfrPOzMzo7t27VVXTVrv/xGap62c/+1l94hOfqO94xzv0wx/+sP7O7/yObtmyRcuy1C9/+ct3/QNxFNostb33ve+t3W5Xu92uvvjFL9YPfehD+uIXv1gB/amf+qm7/oE4ymyWun6nj3zkIwroW97ylrVf9DFgM9X1N3/zN7XT6ax+7QTor/zKr9y1D8BhtimDp4NftBz0kpe8ROfm5lb/+2Dw9LSnPe12z3nRRRcdMiciBU9H1sEafuev0047TT/+8Y+r6q11vf766w957Ete8pLVL1L37NlzyK9+v6/Pe97zVFX1ve99rwKrz3fQ7t277/QL9swzz9RHPepRt7v96quvVkDf8IY3fH8fgKPYZqntbaXg6XvbjHVVVb388st1enpaH/SgB6n3/vu7+KPYZqvr5Zdfrp/4xCf0T//0T/Uxj3mMPuEJT9Bdu3bdtQ/CUWiz1HXv3r06Pz+vr3vd61ZvS8HTd7dZ6npHrrrqKu10OvqIRzzi+3r80W6z1PbMM89UQF/4whcecvvBHwpceeWVd+GjcPTZLHX9Tk972tM0yzLdu3fv93fhR7nNVNd3v/vd+ohHPELf/va364c+9CF9znOeoyKif/AHf3DXPxCHyaY71a4sy9V5SwfNzc3dbq8ktPM+vtO5557L+9///iO2vuSO/eEf/iHnnnsuzjmOO+447nGPe2DMrTs9nXOcfPLJhzzmqquuYnFxke3bt9/hc+7evRtoT0GC29d727ZtzM3Nrf53CIE9e/Yccp/5+XnyPKfT6VBV1e3ex8F24k6nc2cv9Ziz0WubfH82U1137drFYx7zGGZmZlbntSV3bLPU9Z73vCf3vOc9AXjGM57Bwx/+cB772MfyhS98ARFZ62Uf9TZ6XX/1V3+V+fl5XvziF3//F3kM2uh1vSNnn302P/ETP8Ff/dVfEUJIfx9/Fxu9tge/7n3a0552yNt/+qd/mre97W187nOfu8Pvs451G72ut7WyssKHP/zh1RE2yXe30ev6F3/xFzz/+c/nyiuvXF3HT/7kTxJj5Jd/+Zd52tOetiFqvOmCp8P9D5iIoKq3u/22w8qTu+6BD3wg97///b/r24uiOOQFDO1Atu3bt/Oe97znDh/znQHk93LjjTdyxhlnHHLbpz71KR760IdywgknsGPHjts9ZufOnQCceOKJa3pfx5KNXtvk+7NZ6rq4uMijHvUoFhYW+MxnPpNeq9/DZqnrd3rSk57EC17wAq688krucY97rOn9HQs2cl1POukk3v72t/PGN76Rm2++efVt4/GYpmm47rrrmJ6eZn5+fk3v71iwkev6n71eTznlFOq6ZjAY3G52ZtLa6LU98cQT+cY3vnG7YcUHv4m+ox/4Jxu/rrf1N3/zNwyHw9VB2Ml3t9Hr+pa3vIX73e9+twu/Hve4x/Gud72Lr3zlK1xyySVren9HwqYLntbiqquuut1tV1555SHD0+bm5rjmmmtud7+D6eNB6Sesd7+zzjqLT37yk/zwD//wf9pxdNpppwFtvc8888zV2/fs2XPIP4zHH388n/jEJw557H3ucx8A7nvf+/KpT32KpaWlQ75I+sIXvrD69uTwuTtrm9x97u66jsdjHvvYx3LllVfyyU9+knvd616H61KS29gIr9fRaAS0QWNyeNxddf3qV79KjJGXvOQlvOQlL7nd859xxhm89KUvTSfdHSYb4fV6zTXXUJYl/X7/+7mE5Lu4O2t74YUX8olPfGJ1uPhBB8PjtX7TnHx36/Wafc973kO/3+dxj3vcXb2E5A7cnXW95ZZbDumOOujgAPONcuLopjvVbi3+5m/+5pAuln//93/nC1/4Ao961KNWbzvrrLO44oorDmld++pXv8pnP/vZQ56r2+0CpOOc70ZPecpTCCHwm7/5m7d7m/d+tRaXXHIJWZbxB3/wB4d0r33nF7FlWXLJJZcc8uvgi/RJT3oSIYRDTkesqop3vvOdXHTRRZxyyimH/wKPYXdnbZO7z91Z1xACT33qU/nc5z7HBz7wAR70oAcdses61t2ddT3Yen5bTdPwZ3/2Z3Q6nRQuHkZ3V13PP/98/vqv//p2v8477zxOPfVU/vqv/5rnPve5R/JSjyl35+v1O7d9QPs19N/+7d/y8Ic//HYdAMldc3fW9ilPeQoA73jHOw55zJ/8yZ/gnEsd5YfRenxNvGfPHj75yU/yhCc8YfV73OTwujvreu655/KVr3yFK6+88pDHvO9978MYw73vfe/De3Hfp6O64+nss8/mR37kR/jZn/1ZqqrijW98I1u2bOHlL3/56n2e85zn8Hu/93s84hGP4LnPfS67d+/mj/7ojzjvvPNYWlpavd/BL3j/8i//knPPPZf5+XnOP/98zj///PW4tGPCxRdfzAte8AJ+53d+h0svvZSHP/zhZFnGVVddxQc+8AHe9KY38aQnPYlt27bxspe9jN/5nd/hx3/8x3n0ox/NV77yFT72sY/d6WPVL7roIp785Cfzile8gt27d3P22Wfzp3/6p1x33XW3+0c3uevuztoCvPnNb2ZhYWH1J3Uf+chHuOmmmwB48YtfzMzMzBG5zmPN3VnXX/zFX+Rv//ZveexjH8v+/fv58z//80Pe/vSnP/1IXOIx6e6s6wte8AKWlpZ4yEMewkknncSuXbt4z3vewxVXXMHrX//61EFxGN1ddd26dSuPf/zjb3f7wS+q7+htyffv7ny9PvWpT6XT6fDgBz+Y7du3881vfpO3v/3tdLtd/s//+T9H+EqPPXdnbe93v/vxnOc8h//7f/8v3nsuvvhiPv3pT/OBD3yAV7ziFWlb+2F0d39NDPCXf/mXeO/TNrsj6O6s6y/90i/xsY99jB/90R/lRS96EVu2bOHv/u7v+NjHPsbznve8jfN6Xa+p5nfGHZ1q1+v1bne/V77ylXrbSzl4qt1rX/taff3rX6+nnHKKFkWhP/qjP6pf/epXb/f4P//zP9czzzxT8zzX+973vvoP//APtzvVTlX13/7t3/TCCy/UPM/TCXd30nfW8I58t7oe9Pa3v10vvPBC7XQ6OjU1pRdccIG+/OUv15tvvnn1PiEE/Y3f+A094YQTtNPp6EMf+lC97LLL9LTTTrvTpwGMRiN92ctepscff7wWRaEPeMADbnfCQHKrzVTb00477Q5PpQD02muvvbOXfEzYLHW9+OKLv2tNN/g/betis9T1fe97n15yySV63HHHqXNO5+bm9JJLLtEPf/jDa7reY8VmqesdSafafXebpa5vetOb9IEPfKDOz8+rc05POOEEffrTn65XXXXVmq73WLJZaquqWte1vupVr9LTTjtNsyzTs88+O53y/F1sprqqqv7QD/2Qbt++PZ0C/D1sprp+4Qtf0Ec96lF6/PHHa5Zleu655+prXvMabZrmTl/vkSaqdzBZe5O77rrrOOOMM3jta1/Ly172svVeTpIkSZIkSZIkSZIkyTEpbb5OkiRJkiRJkiRJkiRJjogUPCVJkiRJkiRJkiRJkiRHRAqekiRJkiRJkiRJkiRJkiPiqJzxlCRJkiRJkiRJkiRJkqy/1PGUJEmSJEmSJEmSJEmSHBEpeEqSJEmSJEmSJEmSJEmOCHdn7hRj5Oabb2ZqagoROdJrSv4Tqsry8jInnngixty13DDVdeNIdT06Hc66QqrtRpHqenRKdT16pX9jj07pNXt0SnU9OqW6Hp3WUtc7FTzdfPPNnHLKKYdlccnhceONN3LyySffpedIdd14Ul2PToejrpBqu9Gkuh6dUl2PXunf2KNTes0enVJdj06prkenO1PXOxU8TU1NAfDQ087EARIjoDgRMmMwYiAqBnCTwFFEQEFg8n8GUPiOWear/yUgCEYMxljq0FA3Dc4ajFgGzZiedcxPTRFD4MDyCtYKpcsAqEMAEaoQwBiMtcQYUVUym5EZgzUCGrHGYHMHYvB1g/cB4xxN8DQ+IKp0sowizynLAgV87YkxYoxFnGW8soIPAR/b95vnOcYI1lhMkWFchs0yxObYrERVUQQvEINS+zF1VVMHjyI4yXHO0gyXicETYqDyNV4jjUQqjTQKNy0usmPf3tWa3BUHn+NHeDSO7C4/X/L98zT8K3+f6nqUOZx1hVTbjSLV9eiU6nr0Sv/GHp3Sa/bolOp6dEp1PTqtpa53Kng62L7WtY7MCBYhhoABrBEEQUQRIMMAEdMmSauPFzEoiurtwycAFRA1IO3zOpdjogKKqGJEGIVmEnQJdZExqmvUB6yzZGKwztEtSkYhsK8aEmNkyuU4EawImTWgggGMCsYajLOo9xhVxFjKPCO3lsw5ijzHGgghQFSCD5Sd9hM7CIQYMQrWGJwIgkEiOAwuyzDWYZ3DWIsYAXEEVWqtwBoCgiioKM4qGR5jLbV6FLDW4oMSRAjS3teIOaQmd8XB53BkOEkv2HU1eUmkuh5lDmNdb/s8qbbrLNX16JTqevRK/8YendJr9uiU6np0SnU9Oq2hrncqeDrIipJZh8bJ7xqw1kLUtlspti1OotwaOq3+sQ2PlHYv4G3W2d5PASKoEDUgxpA7R4wBVCmsowoeHyImeJwYIkpAIUYUCCFirKOXFzgx7K0GGDFojEQE72P7nKFBNBKaSIyKTMKcImtjMw2Q5T2mtx/PeHmBld07GaunYzM0BupxRVVXLDc1AeiaAqMOI20IpRE0KpEAsYEoYB2YgA+B9g4QVfGTP3tfowFiaAgx0ASPj+31RVViVIIGgsa1lCxJkiRJkiRJkiRJkmTdrCl4ElVyBJfl+BAJjWLUYASECMKk02myrY421Gk7ntrQSUTQSSJ2MBeTg/dWvTWUigExFmMsqpECy3SnR1aUiG+IKHlt2/cXlTzPaEIgRo91QqfImKOLhkgIHmctWZaBaBvweMVYQQTy3CFisGLIjKXoT1F2pjCS0Znejjbg9+1gedBu7xOEOgQMgqoyqiuaECizEmcjISoBweU5eZlBNtmuV41pqhG26BIbTwgNITQgig8QEYSIj56gk9BJ2u4sK22gZW7fLJYkSZIkSZIkSZIkSbIhrSl4MgpEMJmhcBkBCL7BOosGOBg2GWNQFVBF0dXtYdCGTNaYQ0MmJoHUbbbgqSoaA8Y4jDGYyTYz4xx5NydvchpVmromzzKcy1BptwBGb7BWqENDpoKzFo0R75v2zwpqItZlGJG2awsh1h6rjsxamsECoV4m705hM89Up0MuMKjHBI3kLsOpgpg2VIqBOjSotW33Vd0QVQhhiFFL8BWhGmMEfFhhpR5Q+UiIkWgUg23HYIVAjLd+HAxtUGc0ti1saWh/kiRJkiRJkiRJkiSbxNqCp0nwoRqxJkOcg+DRENtwybbdR22XE6BK1HbAt4gckpmICEYskdgGVJP7ACgH9+pNwigxWGMRMcQYETW4Imd+dpbhYNAe3WcNZI5qMAIgaqRrM7z3eFVK54gxUsdIZi1FkWMzh7OWWDUYEUQsxhpUa/KpDjbL0WYEsWq3zKEUWUETAuPoyVyOiBBVsS7DByX6AMaioqgIc8ediPRn2XPTdVQ+YNQTYkMTAz5EPEy2+0E2CesQ087Pot1CGCeBnJnM00qSJEmSJEmSJEmSJNkM1jbjyVisNTjn6HY7+MpAmMwcmpx0d+tesLh6Ut3BrXbAIQETgKig6Hd0PK1OJW+fSRUjbfASvMcbkGgghLY7KQSyTkkMEWUMxoAxOAPioGpqRr6msK7tcHIOay15lmPEEB2IODIHEjx5WZB3e8TGU9UV6tuT64xzEBSD4AA/GWyu2g4C7xY5dVUTQsBZizGOvTtvInAT46bBhwrRdg5WiEpE26HqImDauVAHgycRgw/tljuFdjh51EO6wpIkSZIkSZIkSZIkSTayNQVPWd5pt7VZg4SAxEB/2zbq2hNHY/ANMdSgod3OBquhk0g7q+nQgedtp0/b3wOR2B54J+2wckVgcoqdEokhItZhsxybW0xm8SsRiW2HlBhBBSLgrCH6QFSlYx0LTUUVPLNFhyYGXDA043q1iyjPDUW3izYeMEQfGa8sEXwgGoMpC+K4ImigQYhB8KFpZzBZi0EJRPKyZDBYaTM4l+GbEYqdzHMKBA1gDFGVZhI8IUJnqk8YeXw1Imqk1kBFxIslaCSgeCJeUvCUJEmSJEmSJEmSJMnmsMbgKafIOzgL4j0WS1YU5J0pKjciVAPEW5qqIgbf9i2pIJMgScTSbqQ79GS2NmRq+5yMgTjpgBJjV4eOCwZVg2KJCjp5e152aMKQ0PjVE+DaLYGGoBFjDCEEplxBiB4jQIjUlUcLR6fstJv6cocpu0hpqBb2U4/HBGugP00MDb4ZExqPr9sgymbt7CkfPAdHqUfvsZ0OJnPtLbHCGghBcdbhoyeqEkJArAExNLSdYvsWFvAq1DFQtT1g7Ul86icfpXZ+Vkw77ZIkSZIkSZIkSZIk2STWFDyNVpaRwpM5h8SI+oAd17hCyAqDidlke5wlNBXoZH4TkagRwdDGTGYSNsXJDCdWb4dJSGUM1jlUlToEQgw0IaABchwBRVA0d+SUVKMK3/g2pDLQHrUnqBrKsp31VNeRqEzWLxhboFmJdSDOomJpQqSOSjMaIr0esaqJGCAHFzB5pK4bxFqczQhjJSq3bh8MnmgcK+MRCOSdPtaB+kh3aoqF5WWiegKRoUZGMeBj+xxhMgfLGNvGczo5IJB2Bx6aZjwlSZIkSZIkSZIkSbJ5rCl4UlWapiKEBlGlMDnjpUXKfh+TOXw9xiLkZQa5w9c1oHjf3j9GnWyb03ae0WT+UxtWtYO1ZfI/RfEhtIFQNKgBUBrvaZoGkUiWWRBBnMPagNYNAKKgAnmngxgLwZMZg7WWuqraa0HRUOPHQszbYGowHGGso24axnWNKJOuIyXGiBVBrWUsNU1dkVuDtTCqa0QDAkTfILGdy1TFiK+G+BAZh0DlK6oY8BIZR2UcPXEyRB0jZMbSn5pisDIgaMAambypnX+lgFh72IqfJEmSJEmSJEmSJElyJK05eDLGtCe5hQAGYog0wzFiBZmcThcbEDG4LGtnOBlBFUIIbYCiipo2IGJycptMtsaJGGLQya3tYHFXFNQhYFXJrMF7jzUHt+VF2inmQpbnVFVNDCARsqJArEV9RmwarLW4GEEFFWFcjXEaCU3VznKKgQgEhHFTo75BFIZRqbl1Tc4YPB5tIlbbTXEa2+2DGY4MYdTU4AyEmgYYx0gtUHR6nHffC/mPL34BaRS7OiNKOOnkU7ll9y7EgMVO8jhpd9mZ9rRAa1LwlCRJkiRJkiRJkiTJ5rCm4Gn15DlVcpuhkxlKPjSIB2vbNCl6xViIGtr5TtKeAOecI2qcDB5vZxZpkHbu0iRgUQXjBINFUIIqxghlXhKagNcIoUGqBrWGxntiiATfzkKqrWGpGlESmXYOo4IPILRDysXlbedVkdM0NeO6QoEmRrwGRj7gTRtM1Qg+RsYxTLb2teu2IhgRnLGr2wmNhnb9IoBBraGKEYyZDAaHrcefwPaTT2Xvnj0YjRTWtfcX4cQTTySoTD62bbgk1hJUISrOGpzLsWbhMJY/SZIkSZIkSZIkSZLkyFlb8GTMateTy9pQJ0bfdikBoW4wWFymaAiATu5v29AKJe924GD3U/BgJyGURowaVMzkNDvBoNjGE4InxEC0BhEDMScaxWNoQqBqInWIhBAZRRipUlcNVVjBGYdzlk6eo2LaU+JoiL6hUlhq6jY8M8JII41RQmzHn/vJKXsq7QQqK+31B2nnMZmsaK9TQtvVJYZRDIyjElRwRIyAdRlFt8NyPWLf176KxoCTdoCTcxliDXNz27jq21fgVgM9abu9jMMVBisOMQ4jqeMpSZIkSZIkSZIkSZLNYU3BE7ShEwaaGJAYMSKYgwFNm9AQA1gLiGAmJ8BZkxFChUUQ185GigEwgohBNRJjQMUSQ2jDHlXEOIgRE4UQaU/EEwga8eMxTQh4VVSEBqERIcs6iDFUCpUq0jQMfZhs5wv4GBjUgZFG6uA5/ZQzKMWydMvN+BDaUIx2+58C1giZyciyAu8j4pTTTj6Dfn+Oa6+9GhNqzjnnHO7/oB/m0//8z/h6mfn547l5581s3bqdhYX97LplJ2G8QmEMYgRRoSjadc7PzVD7hsxZjLVtVxkQo+KyAuuA6ID245kkSZIkSZIkSZIkSbIZrDF4areFGQRiRFQI2s4+MpPh3ZNcCrEG6wwi7RY8MWBcRggNJhpsbjFZDigibdgSNACGGCEqbUdViKhvwAq5czRNpB6PUSLWGIrcUYoQfaSxStcLMSiNwEg9FUpQsGow0p6m5xFqH/BExFiKssv2rdvZd2AfWo1wNsOHNuDSdn8dmSvolH2MGKZmp3jUY34C30SGwxXmZvs845n/nZt37Waq2+e4s87kqmuuZff+Pexf3M/s7BzTs3OEYYkTqKsRRpWp6RnGoxH96Vl2Ly6T5yVKO4A9BsUWBqEN7jQK1hmMS6faJUmSJEmSJEmSJEmyOaxxxhO0UQigEWdzxLbhkpHJeXQCGtth2zG085migGhsT6Br7wBisZmFqIixRI1EHxAjuLxEVfHe44FcHEq79UxE28HiUbDG4JxrT58zkInSzUu8QhOhBMZNoIltl9M4eBoF1YgXqBqPzTJu2rmD0XDA8nBAXY+Jk1lTzlrILKXL6XR6XHDv+3DiCSdzy84bGQxW2Lf/AC5zXHDv+9KfnuHf//pv8LEiL7ssLx4AVTKX0e/P0O/12btnN1P9Lkt796LBk5dd6qrC5jlNUxGjbweYS0bmHBiwYtouJ2vbwe6k4ClJkiRJkiRJkiRJks1hTcFTGy4pofFkziIEpN1dh0ZFFZDJHCYjxMmwcY0ezGSekRiIStTYdkZpRENsZ0FNBokDEAMSAlYUtYbQeGLjkajQNO3zmRyD4JsaC5jcgRg0RLI8o1TIrUdsiTWWelwzrEYs1W3HlDqLGmE8HnLTeEgUpSg7CJMZVCLkLqPbneZe9zqfh/7Xh/Mvn/oUO264getuvIEowkx/ih+41wV88ctf4cpvX4HEiptv3sW4qpmf3cqpp55Cf2qOq6/+Flu2bmFu63GMBzXL+3fT7Xqcy4k66SADOkUHO5mJFTRijMVlFu8Ptl8lSZIkSZIkSZIkSZJsDmsKnpxA5gxq2i4j49xqx9NkYjg2d4i029o0bzucTDuyCOPaLXXqIxqUZjgG7zHWINbifcCEsHp6npjJzKjGU4/H7ba7oBjaTiCiIgEy47Cm3QYIYKIiVtHMkrkcHyPeR1QD1ghd63BiGGpkoBFVQTJL4SzRKzEGxLXbCFVhemaK+Zk+w/37yJzDZI5mPCb4yMn3PI+lYcU//uPHuWHHdZywdY4nPuGpBHV89GMf4apvXU5d1Rx3/Ik0TeCqy68g1CPqGKjrMR5lYf9eNCpGHIJBlclpfobMZeRFTt4R1DucyQ/vZ0CSJEmSJEmSJEmSJMkRsqbgyTrbznGyltg0iMR2tpOZ3OYbnG2HhYt1yCRxEtdu0Gu32SnRgDYeMZOB4UYIIRI1gm8HhRvbbsPTEIhNQAKobzuijBGsNWS5wzqBOmDtZLiUQmYFNCDOoWKoKk9llcZZrComKJGaynuGdU1AkWpybYCzjizLyKwBhaWF/ey64QbOOetcpvtdtmyZRw4sYaxlMBzyH1/8AoKyZWaW+933/vzgAx/EZz7zWeqVJbIY6U1NcdaZZ9LrTXP5N75Go5az7v0Arvn25QStWFxaoI4BHyMm1FhjaAdMgS9q8s48zhiCUVTi4f8sSJIkSZIkSZIkSZIkOQLWvNXOCBgFV+QgihHFiZI5IYjBKIgB56Q9mY7JqXWhDUza0+La8Mk6Q1RBDKjq6mwlJRIjoO02PWki1kLmcsQIxkxOmsvbmUdxsgUQA8ZmbahlLcF7mtqzPBhxoK5Y8J5RaKhju0hjLIVzeFWiKtE3CIr3DaGuCJP1ZlXGdTdfT/zXTzE/P8eV13ybUTWk1+0TNFAuLZIZoZvldHozvPOP38H111xNDBFj221zRZbhI8z2+pzzA+ezv/JUdYWGito31CFSqQfaIeLOCLkUjMaKXVpmquzSACGGw/wpkCRJkiRJkiRJkiRJcmSsKXjyTQO2Pb3OOdvOY4rtFjs/CmRFjqrinEV9hXM5tshBhGhDm+IgxBgIUdvtdAKoIjEiMUxuM1gUjRERiyksYgyITLqB2pPfXJYTmgaM4MoCHz0xKDEKgchoVLNvaYW99YgxINZRuIJCQLB4VcbqEY3t0HSxBA0EJkGUtrOnBGFxz14u27sXZwyKMDbQVGP27d2DqOCsQaPnHz/+d+SuDeWcdXSKkiLPuOpbV5BZx0N+7KGI6fGlz/wTW/p99u4fYa0jI4I4qqZCoxJU8KadcTUcDol1xBTtEPYkSZIkSZIkSZIkSZLNYE3BU1QlhIChPV1OsFjnsEWORo/6gLUOayyqHtE2zEEUYy2qtJ1MhEmAIoi2g7wFsEr7fM6iUXEmg8nophgCBEWitjOgohJjTWgaNEZGSwPqpqaZnGhXRW07mbKM+bwgxEAdI1EEhfZ0uxgggqhi2hHpbdcVEDVigW15F1VB1VOqIYuRAOwjUNU1UQTjHERLjBHravI8wxqDNUIIHpt1ISoLSwtcc93VGFOShYadSwsUvQ69qRm2zsyzsDLmm9++jCCRIAYvAecMIQSa4Om73uq8rCRJkiRJkiRJkiRJko1uTcFTHSKm9tQGssbgjMXYmiLPJx0/oATiuMKIQAbBNyixPckuapsiGUNWFoCgTYPk0p5GF9vOI1fmGGPQ4NHQtOFTlrenuolgrODHNdW4ZjyuCVFpfGAUI15BrUWMa4eQA5F2FlVuhPbJDBICIXicQqTtpmo00MRAICIIXZvRzwqaumbKFhTGYqxhqanw4zEymQmlPtAgiLPtNr0QmJmeISu6jAZL9Kan2LNzF8PBCl/87GcJogxDoB4OUY34G26g0+0RorJ1fo4YYd/yAjE2xCjU4tsChEho/GH+FEiSJEmSJEmSJEmS5Ghkul1uecZ9qGeE095zPf6mHXf7GtYUPI00YqIgEQLgiBgBPFBk7YBv2rlLwUeMUzAGgwMTEe/bk+iMabfOGQHnsBhC8Bw8HM+KQdTgA2gDGiPEup3nNOm6inWkwRBMh1oDI6loRBAjqGm7mkQFFUUBFYMx7bypGCM+1GhsgEhmDEy2sHkgtsOpCCi1RmasY67bwRqLWEe11DAnhkGMNNreR4wjNJ5h9GRZxjlnn8W9zr8/n/2XT3Ng724OHNhPNR5Re08T63aYVVSEdnZWNR4gInTddqZnt1HVNYujJYJGjLTbDvc3DaO6OnzVT5IkSZIkSZIkSZLkqLXwE/fmX371DfRNyZn3eg7nPHODB0/LuQMViqgoSojtoPHYNFQhkIlpt9rZdv7TuB7C4gBrDc45CJ4YAjFEVAxZ5jA2a0+w04CGiNGIzfJ2a16MRB9QkfbEuhBQwHZniKXQDIfU6hk1Y0KMWNN2MwGoMe0A8djOjHIiRAyRyLCu0BARFZoQaSYD0yOKsYZSwYhpPzhNQ7cosQguz/EhgPdMGUeGMowRh1ArBLHEJrCyuMC//MunufLbV7F/z34WFvfjY2hnWR0cuD6ZdYXI6jypqMq3b7gOc9MNhBhRA1EA4zDWEFTxacZTkqwyU1PoPU5j5fQeKydYTvz4LsJV16z3so44u3Urrj+Nv/7G9V5KchiJc+1PdZIkSZIkSZLkMJn/15v4oT/4nwzO8Jz2t+szu2dNwVMxNYUCg6rBNA2Z92SqNCFiPVhpA5vcOnKXkbkMKwZVqEYVRrSdfyQRl2XYokRMhnPtyW/tIO+I2PYxItKecNdERBxqLV4Mo/GYwXjAuBoSNCIiWDe5FGMxxoERfAhkzoJpA6eqqhk1Y0a+ITeGKErtA0rbfZRb4fTeNNN5idG2CapqGqy1RCN4YKWpWYoBr4EKYawKWGyZEUZjYgxoFanr/SwsLMBkUDmq7Wl87fl7gGBFQJWoYTL/KuJ9e7Idk4+HiuBNg0y2CcZ0ql2SAFA/8gFc8FuX8vLtf8Q2W1BIxpn3eAHnvOjoD56+9apTeNMj/4aXf+i/ceYHl+HSK1CftuFudks/cT/m/urS9V5GkiRJkiRJchTxN97ESb9707quYU3BU9nrUWQZoWoIoSGMx8SqJm9C242kEatKE3wbojQNVgyZa0/Bs9bhygJzsOvHOrI8w7p2iDgiiCrQnngH7TwosYYohsFowPLCPurgCcS2+8m2l6CqiHGIde3WOlWsMAmdhGE1ZlSNEBGcGEYxUMcGRVAFRTmhN8P23nS7VTAEgkR8FVhsPL4Z40cDVuqKCk8VI0EVhyCixAYsk1P6aIMmnURMjjaMy6xgRIgx0vYtCUEDbjIsfbIpkIiiUQixDdXasKkNomJM08WTxJ1wPI96/Sf4+bnrgP7q7fe99zUMjIWjPKB98UX/xON7Kzz+GW/l6qet8PRvPpPsj7bQ/ftL0aZe7+Ul3yfbpL/fkyRJkiRJkqPPmoKnbq9PmefEMuB9Q+z10RgJVc14sIyMK8qo7dwnbSOUQCDGSFM3WAxRlaLbIctzxAhiTbuNTkFDwGRZO1R8ctpd1Eg1bti3sBevigpg2uHhogfjGzC2DZ2MzTHQbrnLcpZWlhgOF7GqlMayFGoWmzFhMvB8EluhoojCgdGQECM+BnyMLDUVdQyotMPTM2NRHM4G5sRSGgsCwxipMkMTPLbd7YcXYRQimbFYBR8iubNk1uJjpImKansFbQeW4DUSQsSrIpNuKUXa66ZdY5Ic6+L2Oc4pdnFts8ItocMV9QkshC6Xfu1MztG96728I+7h3W8B0wCclfX53H0+xM4/WOHZL30qO//uNE75m5vx11y3rmtM1m7qsj3rvYQk+a7EOczppzA+fZ7xlgxRmLp2AF+7Eq3S/MkkSZIkSb67NQVPNsswzlEUXQBCqPFNgy89rtclVjX1cEg1GuBqj4kRg+JQnFgcgXo4oAmBTq9HlpegHjGKmcx7iqr4oGiEiGNcVywtLxKMBQyKIta0nU2qGLHEGPDBY2JAQgViMVlJMx4xGi0hooxUOVCP8BqZcTnTYikyh7GWEAKDumZlNGK/tHOUIqCAJ7azokQwmaERwQYlRlhCWQoNUYQG8FGJTLqXpO3aimKoAJWIakR85GB21PZ0CUK7tbAdAOUm2w5ju9dvEkAdHKyONIet+EmyWcWvXcFbHv5IYr/ELA7QxSW08Zwz+g8mae5RrZTb33aC6/Pxe36UcI/IJ36uw8999Nnc4x0L6De/nbbhbRJxxy7sei8iSb6DmZpiz1POZ+vP3MCvn/5Bzs8bCskAuMmP+Nmrn4p7puBvXN8W/iRJkiRJNq41BU9ZVuDyHNTgnCU3Ob7xaFS8eghKCJ6mqqhWlqlXVmA4wniPjQ3GGCzCsBoyFZVuh7b7SWhPeLMQMagrqOuawdIio2oMzuGMRY3gmwpnLGIsQQNNPcZlJZ3eDGIzNCp1VG7Zv5NhvUyInsY6htWIHsJ8ltGzjtxmZEVGUCVoe/pejIEqKg1KpQFshoqbBD6KMUKIbReWoqiG9iQ+BIwgViadXrIaLtEekAdq2oHnaBssycEZVu3vRgR08jhVQji4DfA230RP1pEkxzzV1Y6eY3Hc/n+2kdCK4ZHdimue/Edc+riKF33radTvO46tH72SsHff3bbGZO20qmDyDX2SbAgPvAD53X18/OzX0ZWMrskBS9DIgTjiGj/DudO7+doF96VIwVOSJEmSJN/F2oaLdzv0ix5N085wQiAvMwRDiA0C7XDtfo8wPUM1rmjGQwYLC/ilJQoUh0EERlrDaIUmNPRnpsnV0IzHuLwkiHBg4QCN99iigziLiCE0FUYVYx2YDPWQ96bpdKcITYOvB4ybit3LiyzUA2KIDGPAx4AY8EbYrwFtGvp1hRkLlUZqjdQh4Nt+KiIgxmGtIcbYhj8Kgm+DJYGDM5faLqVJ+4EqBxuX+M686ODtkyRKRNpfyCH3l8m2P2tdG26FOJn91J7sJ5NT+5IkOXblcgctT3fgvkXBv977r6guaHjjy+7FOz5yCWf9xQHiZVcd9XOwkiQ5DIxwy/tO44k7fwGJyv57ZsQMuruUmW+PyK/dTdi7j6L64nqvNEmSJEmSDWxNwZOvKnyWUZQF1mT4EIBACB436QzKnCPGQJYJZadLNe5Q9PuMVpapVlYYDkZkTUMukZF6NFgWFhboT8+Qd7o0oyGLy8vUGpEsB+twLqf2FcPlZfKyQxRHjG1M1FQVw+Eyi9WIxaZm2VcMfUOzOh8JjFiyvKAWpakbRJUxEY2xPS3OGg7GOwdnmiPt1jaNq4nQbXbwCEjbuQQyOXHu1vyp/W3yRHLrb+3w9NW3gLaPbeOuW5978rTtHY259R1LCp6SJFm7QjJ+ectV/PKzruJLT6t51qXPYubPp5j62NeJw+F6Ly9Jko3q819j6+dv/c8TPnrom9Mm3iRJkiRJ7ow1BU/GWARD03i8iQiKMY6y6ICCj74NoVwGGEQUYy1ZKOh0ezSzNaPhgPFwhcHiAq6uCfWYPIKvG0yZMxiNEWOwLsM5JWpkNB4yHg8Y1CNMaAgrS3ijNDFQx8BSU3OgqYkC8WDHErRBjwgYS5bnVHWFsaadwTTZXifGoAjGSruFLt66cUcPtioxCXxWh6ZPpjPdpmNpdcvcbR87WQMH13ObLGryRg42Td3mHa0GWG13lU5mQCl3sskhSZKj3F2Jny8scr5+0Xs5cP8hL3/5f+XL73gQx33oW4R9+w/b+pIkSZIkSZIkSQ5aW/BkbXsKXZTV+UYaPXUdEWlDmrIsqesGZzOU0IYx1gJKXhSU3Q5VPU0zv4XRYInR4hLjpQFuXFEtDjAuIzMZwTcM6jFew+QEOE+lsT1ND0U9RAGvymKoaTTSDh+n/a5MDMZYrLWIMUxNzxAXF2iaGiUSDwY5IhgxgCB669aTNkRqu5FW76M6CYBWE6j2QD5jDnkctMFT20EV2216cmvidPA5VnOt1fBpsvZJe5UABgMHu6JS8JQkyWEyZ7v88SmfZfjrn+LtLz2XP/zoozj7zw4Qv/GtY2JAe5IkSZIkSZIkd481BU9NUxN9TpF3UCwxekSEGANgCL6d/WSsAwRrLXme430bmoToMcGACEWnDaHC7BbGwyHDAwdoFhapRkNM3RBjxNMO+lYgKoRJR5NCGz6p0MRIFSJhEiQZ53DWrHY7iZh2qLl1OJdR1xVMQpzVjqVJG5KqYIxhZnqGEDzj0bg9iY7bJj7KrW1KbTgkqujBAeEAk+HhbceSRSb3X91ix2Sw+CScWn1mbU+vWz31Tg69/21/T5IkORy6Jufn567j55/+Vr705JpnfPnZTH9wirmPfpOwtLTey0uSJEmSJEmSZJNbU/BEUHwd0DBERLHGYlxGnnfQ6AFtA6OmBhcx2nYg5UWBWMEHi1RCjO0JcUVWEDRi8gxXlHTn5xgtrzBcXGCwvMSgqhiHQJxsb4tMToy7NY1ZDaX0tvGQCMa0h1KrKjEG9u3bgzG3di0ZabfcRQ2rOVK7+07wTYP3DY6DP/hvz6mzOulAgrYbanLCnEEQsavBkp+sNU7elxpBJh1MYgyqELXdEHjbcU4qt3Y6rV7b6jUd/C0FT0mSHBkXFjnfeNB7WLlozP/55Qfwob/+UU7720Xk8quJ4/F6Ly9JkiRJkiRJkk1obcPFQ4OKR9UQYhuZhLpGvMdah7EGa9vAJwRPCEqIESOCxUGIGGtxRd6+c2sxGrHW4MuSelTgipzOzAzT44rlpX3s37ef/csrVH6yDU7a8MaYSQR0cOvbwangUScdWAcHcSsxKk1T0yk6iEaEgIU29ArtlkErpj05TyHUNZlAt9PB+EDWeCREOgK5WKwxWNOeeAdtWCVYvLYn0kVrCUXOuKkZjcfUGgko8WBKJkLQ1SQJuPUQPD34Z1UC2m7lu80dJG2BSZLkCOubkt/a/nV+4/lf5brnDPmtnY/iK+/9QU78y28Tbtm93stLkiRJkiRJkmQTWVPwJCL4OuC1Bgy4nCzLQZUY2llGLnMYY27dImbaLWVEJc9zRuMxxrRzn6yxWFWwDmkaKBSXu3Z4eVlQ9Apmt21j++Iie/fsZWFphWFdE0NEoiLWgirOmPaEOpn0CEWLcnDulIBGjFjGwxWmMmFLkVOUObGcYTjw5J0Oy4uL1PWYblZSCpz3g/elCbDja19n2pRkMdB1OdYaGt/gm4apTkm3k2FcjqoQVaDssby0wLga41VoJMMT8CI0CJ5IQAgCDZEGJQAeIagSRNtthDHeOqicg+FT6ndKkgTs3fQ3gRXDWVmfd576Gapf/n+884Wn8//9049z9nvHmP+4HG3qu2UdSZIkSZIkSZJsXmsKnkajCsFQ5jnWCMYIjW8AIc8MWZ4DhhgV53KMCCoRRfAhgAjWWGQy/FvUgIloNGSunRUlUdp5Sc6RFTkxBjrdKaZm5lleWmThwAH2LywyHI0YNzVmMrg7QwlRESNICBiV9tQ6VeamZzhx+3ZuuulGDBGDMtNxHH9cH9M/iT37h+zHsD0v6OcdFgjs2b9Mvn8fF556GqVxIG3gFKsK6jGdomRmfpqs28NkGXWtLC4NqKsRBV2C9hgNRnhdQUTxCFWM1AhGLI2v8b7BWoOIxZcFwRUcGK+wWI1otB2efjBqCrQzrlLylCTJesx6KyTjhbM7eOET38bVj1vh+Vf9NOO3n8j0332NOBze7etJkiRJkiRJkmRzWFPwNFxaIo5GhM4ULgdnC5yziLUolqoatX+OEdWSPC8Qc3AbnCHGdr6TFcHZDJflGAuNr2gCFK6L9x7xNU1dE6OQ2QwLZNOOoiyZnplhfnmZpaUl9u4/wHg0QmMEjViEQgyZtXSKEm+EIAZfN+zdfQtz012mpiwnznahHhBG++l1tjJlI3VZUHSmWHaOpd072a6Re5xxDr3+NJl1rCwcQHfvYKpwzGw/kU5/hqzXAeuoRmN8M6LIoJeV+MJQ1TX9rMAdtw3X6TAYjtmzdx9lDNRVjYSKqSKjcG0JPIHxeEQRYD4rqYJnRT0RaBQabTujctJWuyQ51o1i+N53OoLOyvr8073+lgOvH/I/XvoYbvi9C+h/+Euo9+u6riRJkiRJkiRJNp41BU+FDwhCFZYJRYEWEBtBrCN6j80yrMtwmUVjg/eR2IBzDmszIrEdrB0C0SijagUjQlTFONue6ibgXIYxlqauCd5jHdR1RERwec7c1nn609PMzM2zcGA/B/bvp6kqZnFsywvKbpdi2zZu3H0LY++xLuO447dyz3ueynC4j1hXDPaPqeuGrBoyHlYcPz3FYHmIHazwA8Dc9BzVLTsZ37yDbn8aF0acfuJ2pqZmMSajM7uVAAyXF6mWB8QQ6LiM6BsMhjLPKLbNYrpdfBOILDEzG6mrMeOwjLUd+p0OZb9HwDBYWUEHQyRzuBgIvgEfwFmIod3qJ4Ysfq8qrZ2dnYblGtb5m9kkSe6cR33gpbzmaX/Dj/d20jfluq1jznZ57xmfYucbPsLDn/58Tn6VEr96+bqtZ7Oz01MwaNLfxUmSJN+FmZrCzM+igyF+z671Xk6SJN+DO/44/KnbiaaBz314vZeTrKM1BU8nGkdRdmmMYRSUelgRDOAsoTFYl2Gdo3EOl2dYZ8iynAalrhustWR5jip4X+FDjTWunR3lfbsFDyYBlKHsdFGNiBiKqmFUjVgZLBPqGmMMnU5B5rZSFF327dvPsBqyaAXT63LyWfdg9vSz2LN3N8tLy8xO9Tn5pDO49voBN92ym/FCzaknzmOLGnxDP+sy1xEKVcxoQOFHlFnJcHk/ZQHHn3Qy89tPoCinaUKk0ch4ZYXR4gKxGZM1bfBWbpnH5iXGOhBLUGVhtI9qsALVkHw0YKZv6U8fj+tMQZbjQ0RsRhBHzAuGwyHDcY1oRTdEOmJx1hI1csPq5KfD54K/W+CLw3uy45vHMf81YdtndhGuuSF985MkG9Tpr/x33v0nD+YtF57Ejh8zPOKHL+Xnt/8T52a9dVnPCa7P1y96L2c+54Wc89J1WcJR4T4fW+CfFy9g4etb2f7FyMyXduJv2JH+Lk6S5JjmTjuF6sxtXP+Igmc++lNc0v80/7B8AZ994llw1XqvLkmSQ4jgzjiN4bnbuP6xhtf+1/fx4PJmdi4pDzhvvReXrKc1BU/9Tk6vLNHMUYugGKqmpvGBUd3QNA3icoIx1CMFa9ouqDzDTQaRq0byoiDGiCCTYKkdEm4QMuuIqsTJ6W0hKtaCKzIQXR1WHhrPcLhCjNDtT9F4RzUcMPBjqlHNCXXNWefcg2AztveW0WqF666/nv37hphosNYwrsYUDu519j054dTz2Hf9NciuG+jOzJJ3OgwW9jFz3Dxbtm5ldn4bnd4UsYkc2LsLNUJelGRbttKMKmJTUfb7UOQ0IeB9YHFhH0uLBxgPhmQa6Pe75HPTdHpdXLePyXt4HxmORsSFAXE0IjZjyqCc0u9ymvTIsgIyR4ieEDxXjA7/LJUXbfl3zjjrcrgAmqcE/r0SXnXtT3DTP5/Cif9aUXz1WsK+/Yf9/SZJ8v3zN+2gc9MOzv4wXFuWvOQHns/Oh84w88idPOe0z/KU/k10TX63rmnqtMW79f0dbX5hy5d57ZlXwP1g+PSaf69K/te3fpLlz2zn+C9UFF/+NmEhfYyTJDn6udNPZeejTuLAAxre/JA/58HFfuZsd/JWy73zS/nEmRem4ClJNgh35unsuuQEFi8e8yc/9Kfcvxjepiu/T+nSOIZj3ZqCp6nZLRTdKYJvsL7BGEPXCLZj8MEzHo1YGY4ZxIZgLaYsUBXqumEUlsnLkrzTIXhP2elgxRC8p451e8odDh98e2CecyBgRQjeo0J7gp11xBjBKJ1Oh7IAFUdT78ciOJ9TVWO+etllfOvb32YwWuEB97wXB4ZDzO4RWe4YrjSUWYF6g4tCtbzIcN9e4nBAJoai0yfUI2a6BVtn55manaM/vw0Vy/5bbqIwUE5Po2Q0tSc2Qzq9PsXMFiLK+MBeFvbsZri8BHVNx0DRKcjLHnlRkHd7mKxk5CPjuqauG6Jv6JUFee6QqDRNhRGLKTpkZQ+TO+roKXbdfNg/CQqxq3/OxPLDJXziBz4CPwB7//uADy6fyxv++nGc9ae7iddcn+a4JMkGE8dj+Mo3OP4rwBuF92+5gLf8+BNZfNSAX7/vR3lCb+fdEkKNq+yIv4+jmblNR2vX5Dy0E/n8fT8I94W9PzfgzxYv4K1/9wjO+bN9xKuuS6cKJklyVBr/+AN50u/+Az83+zfYg9sh6B5yHytCdOnEnSRZdyIsP/Uinv+qv+K/Te26zWv20FEQMc0pPuatLXiamaM/Pcu4qgjekxloBgNUoAkNmUDulWJYs7i8hHcO0+sSjQGBpm6oB0OGmaPsdbFZhrOWPM+JwVMFj046nWJetDOdnMXSPl4yhzSe2gcEJXMOI45eZ4o49FSdHguL+8ltxrAasXTgAMYK1+24kRO2znHL3lsoihxxJU1QerZHL3Q4cesJhPGAMFjC1DVjhvT6GcdvP4mey+hOzZAXPVYW9iNaMb/tBLAZzbjBmEAxN8NoNKZeXmFleYFxNSAXwaO4sqA/P0/R6WDzAjEW7yNV0zBcOMBoNCTUDX0bMVNdjDFYK1jbx5UdPIa68tSDijBaIVbNYf8kMP/J9r2ttscLZ3fw35/1h3zxacqvXft49r//ZI77zD70mhvab3iTJNk4VAl79zH3rs8x96fCe4+7P295yBns/BHlBT/2/3j27KVst0dmS16zq/u975R8X7baHv9z/hpe+t/+kEuf6nnNjY/h2r84h+M+t4h8+wbi8vJ6LzFJkuT7I4I9+wyWz9/Gjh+Dv3jsm3lgkcERGC+RJMlhIII7/VSW730cNz1c+Ohjfo8fyLv8Z6/ZyBEYVJxsKmsbLt6fptfrY42FqAgR6XZxLqMaj6lGKwxdRmeqS76wyIGlRerlIcbAuGmImW3nQBU5g9EYk7czoWyRURQFeZaTZe1PzENdE2KgNpC5HGMMxggW6GQZ5DkxRmKMbJ2aJh81LI9G9IywPBhQqDJG206slWVuaiqoGrpFn9IEtnZyLjjlXI4//hSszVlYOEBx0slQVYxuvoHZ+bOYO+4ESrFo5QlNw3hpgdltx1PMbMH7QFYAojRNw8poJ8N9O8lyodfpUlUVrjcFIZKr0MkcrtfDuJyVxWWapb2UsaFTZBSzM+SdHjbPiRqJwdNUNWostfeM7YgwGpObNohbD1YMPzTphGp+PXBlU/PWPQ/l7z9/Ead9NNL5wrcJBw6sy9qSJPkuVPG7bqH//ls45/3wz7Mn8on7/ijXP6rgRy6+jOdt/2fuXwQKueudSitxzAmfPQxrPoZl8r3/frdiuLDI+auzP0H4lX/gBj/kPYv35x3/9hBO/H+G2X+5Fn/L7slpskmSJBubFAXXv+JC3vaMt3D/vJ5056bu2STZqExZcsMv/CC//7y38aBiNHnNfu8fPDrW53vYZONYU/CUFyUuyygVDIIIYIUiy8ncgG5RUBY9mhDpz22jv38fu/fup/IVMQQWlpYYa6Q2CllBpyjplAUuswxthssysjwj73TIsgwxBjfphgoeYlSMaZPUzFisNWyZmWdL3qHXn2KuKPAz0+w/cIDlPGM0LhiOBvRnpzHWMJ91OPPCC/nmly/l3OlpTjzuFLyHlYUDVKMBs1u2gI9kKGXRJXc5xmQ0KwcYrSxRFgVZ0cPaHBEQUTDgxxVWPbOzs9huBx8NbmUJkztyl1GPR0ijGCz1cEwzXGGq36MzexpWHBo8ea+LK3sEgaYeMh4OaKoahkOauqHbm6XJ8nYL4jrLxHJe3uHNJ30BnvgFDjx+yJ8u3Ys3febh3Ot3b8Ffe/16LzFJkjsQFhaxn/4yZ34admY5v33yE9j34BPY/V8bHnfBV/m5rf/8fQ8of+2+C5n5pytJY7DvPlYMZ2R9fnXrFfzq465g+OM1fzfcxmu++WjyD8+y9YOXpU6oJEk2LNPtcsXrzufrj/u9ySyYu3cuYZIka2Omprjidffk649Jr9lk7daUYmQum2x/czjnMAjWWYwImheogKhQRaWwfbpTs/TntrJ7z27c4iIhRpwfM6prVkZDdg8WMWLouZxulmFdhstLym5B3ilxedEOJs8ysixHMER8G74YS6aGbZ0+nSaQ5QUFYIuSjkLT7TEaDRmOhyytLHPiluPZetJJzE3P84Bzz2O2N8X+XTtomobB8oBemeMrz3i4ghVLVnTIp+YxU7NUgwozrpjetgWyAskKXOaQzNGMxizv2Y1rRpS9rWAyYlOTZzmdqR7OWTpVj+X9iwz37SeEmm4m5P0ZTJZjJIOig1iLr2q890QCBCF6iCGgMVL0unS2TOOuXZ9Tq/4zc7bLz89dx88/7u384gN+kMv/27mEb1653stKkuQ/oU2Nv/Z6Zq69npn3wLcmA8pv+eEZFh8w5sfP+zo/Pf957p2H7zkfqtKGv33bxWzf92930+qPToa7Nq+ka3Ke0l/kKQ98H8P715z3wBdxjxd/Oc3lS5JkQ7rh5+/LNx73Rrqm/N53TpJkfYnw7V85nyt+/PcpZO2v2TTjKVlb+4yCMw5EsdaQ2wxjDTF4NASa2qNiyHIL1mKMkOUO6yzW5RhrGVYDRvWIYjik8JZxaDAhoArD8RgrQ1gyaFESuyVkDpxrQ6gsJysL6HTp96bZunUbJx93Iis370RocGWJD5HMWMRA0esw3esy3+1RjQbs2bkL1yhbth7P3t23MBgtExqlHg+xGoi7b0aHA2bKnCLPkLzAj0Zo8Ext3YoUDrICXIZkGSqRAzd8G1OvUHSm0EZBPTkRzQzOCM5YTKdPXY5xKLY7Td7tkvX6+GDwdUP0HjTix0Ni9ERrMMYgbvJBNzAaLmHrHMLGftG+/oQv82d/dQN//Ion0v3rL6z3cpIkuZMODijf/hXYDlxZFPzmiT/JgQeewK4fUe55/o085riv88je5ZyV9Q957JO//VhOeN/lqdtpA+manH97zO/xX697OSf/3n+kQeRJkmwo9pwzecNz/vhuP301SZLvjzvjNP74yW87LOMZkmPTGjueHJ2iwDceayyZy4hEYlBiBDA4EUyegRHEWIxAv99DNSJGkQNCp9OjXwzpDYcM6zHDuqHWSFQlj56eLXF1DdUYk+dIlhFkQOMMoSzR/jTZ9BwXPeiHaQYD9lxzLRoatPb42rN/uMKBxQWmi5KZqR4YQ96fJcu7RA+7d9zI8u4d4BvKLKdTGvIMupmj059j2/YTmDvlTIjAygqiEVuUmNk5iKCuQDLH8s03YOshNu9QbtlGtBmiAZwgUSF4JCjYDJuXKNCb34bJMryviZNvBGL0ZDZDnKM3vQ2vgXo8Jhgh956IoKLUVU3wh3+4+OH2jOm9nPv6t/CirS9iy598Ps0aSZJNSKsKf+31TF17PVN/CcFYPjp9Jh856yHc+PAZ5i7excvO+kf+4PqHUT7fEA7sWu8lJ9/hBNfn0y96LRcd/4uc8/Ivp/ApSZINoz55lovKJaCz3ktJkuROCDM9zsyWgP73vO8dScPFkzUFT2VeoFERBGccwUfGdYX3NWIsLs8RiTgUjMNHj5mcPlcWjvnZWXzdMKoqsv4M1jg6TU6nrlkcruA1kguUnQ5TnQ4uBsqpabL+FCtLyyzt30ddD7BeKMeeq754KTffcC31yjKdPMP6QFXX9OZm0DiNryuGVUUzbuiUOdN4ZuKQ3MJxW2aZmp5ph6WXJSEEMmMpZ+ZwRQdGntHe3ZjMYlyO9wFZGWOyEj8aAYHFm66jl+cIAWsMeacLGsCCxgB0EIQYAPaRlT2k7BEF6nHFeDDE9nqYbpfRuGa8MmSl8mAtTdPQhJqq8tSjBh8hRCWEzdFT8EOl5U9/5fd4wf6fp/eh1PmUJJteDISFRfjSIid/Cfhdy9u3PIh8eTc+nW65YW21Pf7jSb/HxTe+jBPe8Ln0g4AkSTaEhbMK+lLc4duCRnaEIVfUcyzELtdW27l+vIX5bMD9e9dyQb6LWg3ZYHN8TZwkR4MwnTMld3xqXdDIgTjisnqKfaHPjc08u+tpMhO4d+dG7lvczIpPwdOxbk3Bk7gcZ0sCnqAQg0JUnAoqQlaWhFDjRw1+WFM3VdsJJUIYe4zCzPQU9d6KJng63S5lLOg0DbmxLC8vMpVlZOLwTcBmjnpY0z9uDh8tg6VljG/wywNu+Nql7LnqClyRt8O+i5JMBJtZ2LeXbogMFg+wVAfKXp/TTjieE088iY44HODKLsXWLZi5OaTsgApxOMRkBc24otp9BV7MZIi6MFwZ4ZfHgNA0nqU9N6JaU247ju50F1u0z2FsBkSiRtA2gGqagG8aoguElSGj8ZBquEIzHsPiIgiMhyuMxitEFWqvRBSTWdQ66qqmGg/wdUXTbPyOp4POyzv87G9/kHdf80j0K99Y7+UkSXI4xUDYs2e9V5HcCXO2y/te8np+7qqXUn7k39d7OUmSJKDwpTpws5/mM0vn8pldZ7Hnhjm6Nzj6O5S5y1ewN+1B6xodV2jdcK3t8eWpB6EnbEEzS/b1y9f7KpLkmOEWxvzzeDtdqfj08g/wDzfdk/03ztK7wdHdqUxfX1F8+xZ0PEZHY5g0S3xx6iLYPk+dB+D/rO9FJOtqbTOexBAQFIvGmugrrPdI9IQQWFlaZjgeEUPbTOd9g6pirCVopA6eelyTW8doOKCYnsa5kryucMZQGIsfLKO+QQNkZYGJkX1Xf4vae2KIdIuCueke/d4UzoA0Dc7mZNN9QhPpFjlTc7M0UTlQ5PQXF9uT72bmKbszUFWoCJrn1OMapwazNIQsZ7j3AMNdu6iaMWH/HlQjxlpclkNWIkWJqHBg5/VYiSAZo9rjOhY/qqkWb6EoCrKiwGYZTV3hq4pRVbG0sEi9bz9ZWeJDg6B0ix5FJ8c4R+w46nFBPa7wYqk0Mq7HDIdDTBNw1hKzErHrf6rdWvzM1D5e+bKcs54u6SftSZIk6+S8vIN76S7kYy4NG0+SZN1te99X+Y1/eiI6GBGXlpirrmLuNm9X4Dv/ptIGGI9h8kMP1c3zw9gk2ezi16/kjy/5Magb4oEFto6vZOt33OcOv7qYvGbT6zVZU4rRRCELEW3GxGaMURCE0XjMeDiibhrECN1el6xTghFiiATfUI8rus6w6DKaEDFmGVElz0tMllMWHcqiYCF6tszNEAZjQuPJe11i02B8w5aZHtu2bac/M4MzBq09hcvIel0QS+MD1XCAy0sikf7sPDNTc+SqhMEyY4E8cxixiEQsSn3t1YzHQ1aWhgwX9mJRTDMmyzJsp4/Jc2xZgnV4H1neewvdrqPozCAYmgAhGnCCK8t22HqWUVUBQaibGmka5vpT7QmALqdqGopeSW9uC67IwVrAEKNSHVigairGdYOPkcHKgLoZ02hkaWUJd9cOPVoXv/uAD/F/T3so/rob1nspSZIkG9aRPvHl7ee8l+c/5KW4//elI/p+kiRJvpc4HBLT14VJsnnEgL/+xvVeRbKJrSl4qlZWwAihrhDAaCBUYwwQVOnPTlN2c7KsRJ0lxEj0gegdsWmQrGRmPkOsoWnG+CaQZwVOLFIqLi/AOrbMzsBgQF2NqUKDViPm+l22HH88U9Mz5GUHxg0IFFN9xGXEqDR1hRDZv3Mn3elpes7R7+X0ypLu1DRlZwpXFoRhhbEZagriygLN0jIr+3bT6RYU1tKdm0VchhdDNRwQxhVNtcR4aZHMQmdqiqwsUAxhOEaMUpYl0QoIqFhUa2hG9KenyTp9xDmCb7fadZsxxcwUZm4eigIpSmgCWlVkrqCnyniwwtKBA7gtHaIRmmZMf26W3rcuOzKfCUfQ43sLvOL5J3HG/05fYCRJknw3R3rw5llZn+ueHTn7U6kDNUmSJEmSJLn7rCl4GtcVtigQm2FijXhPZi0xBoqypN+fQnKLtQYVRwwNGgMmKJkYIhHBUOYFU/1pVgYDVJWsWyKqiLXtEG0MWV6QiYC3ZH7MVH+KblGSGUtuLFUzJO+WYAxRhHE1ZLyyjIRAFoTZLGN++3Zya8mzAsSRdfsEhaoeIzYQqyWiBrQsMNbgojKzfSvl7Db8ygrNYIFuvwsaqcKYolvQmZrBdrrEEKmrMS4EcisIgAi+qTFU9KcKrOliZmaR/iy4Ajsc4/1NGGcx3S4UGZQlBEWrBqkbMA4xQlZ26U4FAkJ0lqA9yqamyO94EONGZsXwow/7OjuKAq2q9V5OkiTJMevXHvBR3r/lAsLefeu9lCRJkgSwszPouCKmgzqSJDmKrSl4Mk4gemgaJDZoiBjnsFbabXcCokpUgzGCyzKsMYQYcc7SaCRzjpA5yslJcgAuLzAaMZmF4ZDhypA81FgNWPX0Z2YpshLbBKCirjxEMEWXJnhiMyI3lu70DC5EOragP7uVqfntZJ0uoz37CKKs7NlP8B6NiqIYoxT9KTLjCCGQ93r0TjkbpqYwTU1uQfKcuGcP2d7dlJXHZo4QlXo8pBmMKLt9xFjGgyXwniLPKWbmMFNTRNM+nrxAx55wYAF8DTNTMNVHZmaJVQOjMeI9qgJFhmY54j3OZWgEIwaTGWye4bLsCHwaHH6XVhW/dM2T+PA9PkTX5Dxs7nLe078PIQVPSZIkdyjcDV1IP1jewPvnfxRS8JQkSbK+RFj8mYs48flXM/7Z4+Eb31rvFSVJkhwxawqe6oX95M6RWYuPEcTQ6bQdOM1wwHh5mbzfJ0aDVVBVQl0jCtblGCNYI6gYur0G31QgBmcsmc0Y1mOESGlgujdF9B71NWWnS6ffwxpLbh318pCIUDcVmXP0ej0ycdioFJmlMzWHUcv4wDKLe/fTDFcwGqgHQ5pqjKKU/SkM4Bf2Qoh0QoMYh7qyPZkuz7Dbt6DOtd1I4xqX16gGwvIKzbgieE/TjBEbKYsc2+1gsrw9Ja/fR2JEhxVx3y3gI1RjTLfb/pqehXIaBntgOEQVJMuh18FecE/i179N7iOxrvHeI9aRZxbjNkfwdG4mLIw6fNtH7p3D2657CN2lHeu9rCRJkg3LypEf4vfSq55KuWPXEX8/SZIkyXdn5+a48lfvwV898Y28adcl7PjW1eu9pCRJkiNqTcFTHA2JnR51jORFSdYpwLk2IBLB+xqqCuk4fN1gjYCCdY7oG8RarLEUWY4PnnI0YDSuaOqK7swMth4zRWB+qkeWZdSVp0FxeYbLcpzLMNZi6kA9HDPjCnq9DoWzFM5R9GbIp2aIUVnZv596sEjR69M9fjtEqBaWaEYjsk5JXhTkmcGIEpZXkC2zqFiaqy/DzkxjthyH9nrIli3YLccjxhGXF4kri9i8QHURCQ2Z7VB2e9gsx/S7GFcSosKwRpuGuG8PRgTyAoiAQZwFl6ExInVA64boG7QaYnoFunMBs9KetOcUfF0T6xqZ3oJae2Q+Ew6zrsn572f9K796/ePpu4r+L2SEpl7vZSVJkmxYjiP79/unR4b812aIg+uP6PtJkiRJvjs7Pc3Odx3H5ff/QzIpOaFc5GbXSyeOJklyVFtT8ORDxOR5O1sJQ8Qwrj2oBwy+rnGdDiKCiBBjRA7+BFcjGEtQi8XS7fSoOwPqug2exBiyqEz3+nTKAlt2sIVHlzzGuHYfX5YRMQQRZmZ6q/ft9qawWYbbdhzjlRGjvTtwZU633Ib6QKgivqqoRiMwAradIWUzgzVg8gLpFJCVUNWIFcLiPuLKCnZ+G2bLFuwppyB7O/jhiMH+G8iM0pnbgstzrHVo49FRQ+yWaNWgzTI0FWIV6fZRsYgIOhqhwzFql2B+C3EwAN9A8MQI1U07YP8iThUjBkKDxpp6XCPNLFHWVLJ19UOda/j//v0n6O4UTvjmv633cpIkSY5pz/3I8zn7859f72UkSZIc03Y/5Tw+f//fJ5N2F8PZ5S18uXNee+x8kiTJUWpNKYbrdZAip27AWlAUjQHU46LHxIgVcLYd+B29Ik0AFGMMGiKuyPAaMdZSlj3qwZAQI6H2ZKGmzDPKXg9MhseAWDQqvg6EOCRTZbbfo5sVlJmj25uj3L4dbx0Hrr0Wo4G8LCFE6sFyO48KhaYm10j0DWFxwHBlkaI/TWdmFrf9OLRToDZDohBHA6QeQV0Rdl2P+hGmOJPaZQx27cAagy0KNLSHX3sfcZ1Oe0jQaNxea+7AKsY5onWAoI2CQFgcwLBG9u2HGJHMglEkBmgaFnfdSJ4VFEVJ0Z8hK7vUQWlu3ElYWjgCnwZHxpkOzr7wBszPdgjrvZgkSZJj2A1+hbPeP1rvZSRJkhzbRAiPPUAht47OuKi8jvcffzEcOLCOC0uSw0eyHHv8dvyNN633UpINZE3BUzCOUROxBlQj0sTVsMlmFqslqOCAKIA1RDVtN5C2J79ZEYwxRBWyPKfIcpqqIQxXKEKg6HSwzrUzpEJANVLXY2Jd0Z+eZrrfpZ8XOJNhmoC1lqX9C4wW9lAWHfKiTzMaUHRLup05wmhMtbyIdRbjDLFR/LhCgiBNu4XNRBBTEKoRxgeoawSBvEdYHBKuuY7R5VcxctDvlIQYCCEQqxFhHCEqUnTIOj1c2cNlOZQ5UpaQOUxWEKsxaEAwyHCEH6wgLkNyi2S2HSwewVpD4SzLywfwTRfyAtvrUmQZcWUIR34EyGHTkZyr/+NUzvzW59Z7KUmSJMe039vzUOylVxHXeyFJkiTHMNPv89NnffGQ2050Qn3cFPbydVpUkhxmYg3+pHlIwVNyG2sKnqq6ITMNWIOEya4157ACGhVXdCi6PUzWbiuLdcQYC04mJ7M5qqjt2xXyzFGUJSF4qqX9lLnD5kU7A6nxqAaacUVuDf2ZGaanZ+l2e2Q2xyA04xG7dtwIAv35rVhj0OEyvblZXKdDXBliipLSgs0NZmYG9c1kVtMQGsUPB8iunbB/PzF4yDJMWRK9Jy4uIxhCdAz37SIrIjI9T+4s4iymW2CsQ2MgjGuqpQXG+/eBFWxW4PICKRyu08OUHSQrwUY0RCQq4+EIEzNKWyK0uwlBQATnMsajFerdge7WbWTdKbJOgdkkp9oBWDFI+i4nSZJk3f3dFRdw1vAr672MJEmSY5rkGadk+w+5rRRHPePorNOakuRwi+MxfP5r672MZINZW8eTrwnBYsUSUMQ6FAvWIUCWtcO/Q4woFutyYtNg84zY1CAWQdGoiLVkLsNmGcYaokZC0+CbGnKHKtTjMZmYNmzKCozN8AGCr2hWVqiGK+SdHr2pGUwzopiZxm3dDnlOVIghIt0SIyVSWmTLFkAw3S70Bui+A9i6Jg4X0RUQ51CTUe3di8aAywqkO4MtMjTPkNiQiSACUmbt9asiUmACdPoOje3MKyUiUqNVTRyu4FEQhxoHYrCuJIghqoIYjLWoCOpHoBGXWcQW1HXDgVt20pkdMXXKidjZqSPzmXCkHPnTwZMkSZLvIQw2z3zAJEmSo5WOK25u5oBbt9X96i0PZOrSnaTR4kmSHM3Wdqpd8BAjzhUYASMGVcEWBU4g1J6qadBgyAsLRlDniCKI5oDiRPAhotGgGDJnMcYh1hBjpK5qYpYzHlb4EMnLHpJ18FgW9y+hsUGbGoPijKEMnm5m6G2ZR2anMVOzxNqj+xbagGl2CilyiB5ciXRzyHKQvUjjkWrUZiNVDQYwghOHrxS1Bm1GDEcDHJ5Or8AYkDJHpqZBHIxGbbg21YM6EJsK25tFNSBGiD6gVYXWTfs7Aa+G8WiFykecnWrDqF4PqWp0PKIo+5jQUPsxIdaE0LC0fx+jXp+wyYYlxVPToMQkSZL1dvoZu9u2Wk0/DUiSJNko/nDhFC579j2J16d9dkmSHN3WFjzp5FdUVASxBptlKCDGEWONEUMMAiqoyup9nTXgPbnLEFGCCpgcm3fI7IhaLcOqQoY1Wu2nqRo6ZY+sU4AxDJYWcN5TdnPKLMOglEXB/NY5OnOz2Lk5zPQM6kp0eR+EiJnqIHPzqLGwtADBI42BrMD0p1EiuqAYl6FlO9vJWEv0HougQfGhISwvMN0tcHmO5BlmahrpzxKGY6QsMf0u+AaNQ7L+NLHfbT9QIpgmQDMmjsboKEN8u81ONYII48ESg8zR7/TQLEM6PaxCrMdk1tBERUaBWFcsfetKqj27j8xnwhHyo2d9m53OpSNikyRJ1tEjj/8mn+5ub09STZIkSdaHMWTS/hT5o8OSD7zskRRf/eL3eFCSJMnmt7bee2sR65Asb2dcWwsiWGvxTY1GJRMDJlLVFc4dnEckxBixIlhrwAoSIlX07QwoY6i8pxpVVHVEDHQ6Jb5piCFitaafO7q9Ht0iJ3eOLC8osoxyfgvS70EUwtIQwgBdWMBOBnzbbfP4cUPcsxuph0jlkOkZ7MwUQSLqI2IOzpRageiRPIMYiSsV1XhMzymu20U6JTiHqiEMR+376PRRiZjQoGWnHSJeFG0AFyNiPDFGwGM60/iFJXw9pqkqGl+jGJYP7ME3NfnUNMUF9yIuLOL27CcMlzHBY5qAMqapRlSb7KjVi2e/xfv7P0BYWFzvpSRJkhyz7tO5gX+ZOjMFT0mSJOtIRyP+6ub7MmVHvOsXf4Li71PolCTJsWFtwZOxmKwADCKKkfb3UNeEpsE6i1ePcTkgBG234xljsTYjQ1eDKuNA1dPEQJ7nBISVuiZkgaluiRMwsaZbdpid2YazJU4DvU5OUZa4okS6PSg74ANxYQmsQWNEhyPY2sFs3YZ2u+jiHuJojPgRplugJsN0DNKdwqolLi+jsUGKjGhzpPJgAs1gD7kobsss9PpoXiBFjjaCgXYWlZt8TIJFY42EgMm7qLFIVUMMSDDEKqAxIFmG+hqb5W2HU4jEpmawtJ/heED/xlmybp+8O43XgGkqyBsksxgn6CbbJjFlxu3WxiRJkmTdzJohFOnv4iRJkvWk3tP5OcsHmh+iuC6FTkmSHDvWFDy1oYeiMRA1oqooQmxMOww7yzAmw4htQyA1iExOuFOBzKJ1g3UZIga1jkC7vc25DKKnW+TMFI6pXocsK+j2uvSn5kEFJ5E8z8iLEspuGwJNtsSRWaQowApaNUi3QywK9MZdxIVFGIygGRJ9g3ghDhukU6C+QccVJnNIvw9Y4uIKsjykLHJMJ0e6XchzpCzQvITYtLMyjMHmBdEYdOwhAAG0VsgUrQISLDoKGFcgxqACZbeLH48w9bjtesoEE4W6rtl31bfIZrbSn55Co4DNkLKLVhVhNDwinwRH0jn5bpidgj171nspSZIkx6xtdkSc6q33MpIkSY554apr1nsJSZIkd7s1BU9iDEaUGEM7p0gEgiUSsFkPNRbFtAGVClHabXQmRowx7QlnqhAjKuCMgQgihjzP2DI3w1ynZG6qT8eVGJvRzboUecl4eRlPQMXgtcaaDG08YgwYRcou0u0gGGI2RJcWkQN9GFXEXbdgfYU6Bxh0NEIXFhGbEa1FQgNzfejOgDGI9xidJzrAKAhgDJRdjDhC9GAEEQeSEVeGxJtvxoSGSIQDC4gIofZ434AP2E4P1+siZQeCx6jB2YxQDXGhHbCu1tGsLLFy03UsipD3erjM0iiEKETJwciR+Dw4YqbEo+mn7EmSJOuqENDMrvcykiRJkiRJkmPQmoIno4IY257WNh7hQyArS6QsCGKwxhEQfFNTZDnGGiTSdgYZcM61uYkqakBxWOfAWopOl9IIHaN0p2bpZgX42IZTdU1sKrr9Hq5TYp1DQ4M2HpNZTNHOkjJlhyAZYXqMLCwQr76G5Z03Y1Gmts2DadcUfcBk7Xwq8Q1ipA2wYo0p+8S5WdRl7ducIM61gVmIhHoIvoIoqPfEwYA4HMJgP+QlYnNw7fMaPNIMabxnPFzC7wNT9sh77Ul2MUaCzRgPK2zHoC5HJSMwphp7FpcW6G/dhs86jFdWqKuKEOMR+DSAlTimkIxMDu83JlNGCDMlmysuS5IkObr0xBD6ebtNPEmSJEmSJEnuRmveaheaiGrAmLYjB+dQV8Bkex0KrsgJIeDyDEURsWR5RpZZBEeoGjRGogLWYcSST+ZC5UbJiw5ldwqtKgxQjYaU3S55WWDzHHFZGwKNqzbIyjJM2YdimrhnH831O6BaBgu9XFEgjMaYEAiimKke5AUYhwzHMGzQJiJNA41Hyh4mCCFfQky7jQ+x7eyolQHEgEqGrtTEUKNNwEi7jc4gSF6iWQFZg1XQ8RBVi4+e0XCJvXt20fiIZDnkBb6uCLsbIu3HzuQ9kIoYAjffeAN1E1GNRB/wVXP4PwuAheiZNRyB4ClnvDWnc1ifNUmSJFmLrskYbc9Jm+2SJEmSJEmSu9uagqfgPVVdI8EzNT2DmgwVi9oMFSUqGOuwmYHosSKoNRgMUQxeDBbFWIs2njgZNg4QECKCjxEwkOeAYbi82IY5pm06UizYHF8PqcZDajVk4wZDjh4YUe+7BRkNyHo5lCUYUKHd5jeq2+4oY1HrMFmOyTvE/P9v7+55I0uqMI7/T9Wte7vt7rUFy640LBFCIiMj46sQ8FEgI96UL0FASkIAJDArIfEiENrVToJmxjPtvn1fquoQ3F0JEINsuXs90zw/yZGto3I/vm77qOrUiPc93NziscUu34PUgYNPI5QEVvAyLzOkhhGYMQy8ElIAC1AcHwfq/oA3CYLhDk1smaYBPBCbFSHBdLihv3lFWq+pQHEoeWK6ydRSKRSKO+M8M/Q9GWM4jAzjaW61+6jZnKRuZ4ndtxo1nkREHlFnif4bQY0nEREREfnK3avxlN2ptdKtL6BrKRawEIhAKY7Z8jHlShsjRiCGgLE0YGpxUlw2+tdSoTFCWOZAGWAhMpfMOM2E/Z7hMOKHnsYzdW6Y2gm/eUWtlfnQ05jTti15asgvn9NeXtKlhhCNUiqhP2CrFrtYYSlhmy35MGC7nmCRmp242cIqUXeVWBzyDDFC21EtEN0xCtYlvFnjwZZm2Txh3ZpIBWxpZpUCw4jv9vhhotTClPMX33/F55l5niFnosM6JSKVSmWqlTwPzP3IWJ1iDjEQuoT3lTIOlDxQvRz/p+DEXGc7REQenUcdehYRERGRr949j9oZeZrpuhW1FKwJkBLFl89VX256wwKlVlo3vFRiDCxbjiA0CYuQ+x7HyKVQa6XilDLj1nDz6jW3L18yjwNNGelSoLQrmjRQ80wphc3119heX5NSwmtleH1LnSfi+oK47rCrzXI87XaP7Xv88hJbd4TLLfXVS7xU6LplB1WbIAQ8F7w4Vg2aRGhX+OE11kRqzsvOqSbgk9FstpTUQFqOClIq5IJbJLhTDzMt0HgFd2opNKEnmNMEsJrovYA5KXakBG23YrUaGb1S28TN85fM07jMpmp92S3WvFvDYYtXVi9OM5dKRETupnil3fljL0NERERE/g/dq/E0zxnr1tRaqHOhSWuqh6WlFCOWEjW1GJXogWVT03JGLrgTgFrnZSh3KRCMXDLTNDFNMy/2O5pg5N0NKY+s1h2tGev2gjaA5YGx39NdbJjI3A63GB3uTiUz93uiZ2xaE8aeEAwzqIcBG/d8/YMnbEOLpQ7f9YQQ8BDwbNiU8ejLHKdDD9stli6o/YDj0OTl+F+pEG25DS8mbPXFTXq3eywavl7hw7B8/Xq9NN1CIBYn7Ne008g4HGiaSNu2DDnj2Slecat412HFOYwDK6tEq3iq5FyoxfH6bm0f+rz0XP15/9jLEBF561VO1xh6Xg9sPx1PVl9ERERE5E3u1HhyX/4YfvrpH9lcXtF2iRCMEJadQmYQYiTESAwNhpOi0cSwzGeqBcdpmkgTI3mcmYYBi5GxzvSvbhmngaef/ZXX/Z5l4hOEEAghYGZcXyZKrYTgpCYyZadpjOqBlIx9XziMBapjIUKMXF1fk9rEPE14rnzvO9/l2x98hB/2eN8T1h1srvCcYb9fbrnrIvH9D+HqPern/2D+7C80mw1Ew1JDrQ6pwZqExQRdAxV8f8CoVIBhgpKha7CmwVhusGPOMI9L480L45QpxZmniVIcYiSXzDxP5FLINZNzppTCVGbGcebFuP+3TB7iyxo/+PhH+Pvdg+v9NxfPjA+f/p7qpxmKfi4yy+tzzFwzMyf8P1bu4Ji5/msdZfu4TpXr93/2Q+xidZSa/6l9YXzzN5+Q9bv4jfS8ni+9x54nPbPnSbmeJ+V6nu6T650aT7vdDoCf//aXD1jW4/v1J7977CUczW634+rq6sE1AP728Y+PsaQ3+sNJq5+XY+b6K35xjCXJERwj1y/rgLJ9Wxw717//9CcPrvW//Omk1c+HntfzpffY86Rn9jwp1/OkXM/TXXI1v0N7qtbKs2fP2G63mGk46WNyd3a7HU+ePCGEhx27U65vD+V6no6ZKyjbt4VyPU/K9XzpPfY86Zk9T8r1PCnX83SfXO/UeBIREREREREREbmvd2tStYiIiIiIiIiIvDPUeBIRERERERERkZNQ40lERERERERERE5CjScRERERERERETkJNZ5EREREREREROQk1HgSEREREREREZGTUONJRERERERERERO4p8E+d4rB5gh6AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6C1xao2Udtn1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}